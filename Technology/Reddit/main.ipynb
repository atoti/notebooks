{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2020-07-23T05:31:14.898946Z",
     "shell.execute_reply": "2020-07-23T05:31:14.897998Z",
     "shell.execute_reply.started": "2020-07-23T05:31:13.964682Z"
    }
   },
   "source": [
    "# NLP exploration with Reddit data\n",
    "\n",
    "I came across this old article about [Creating a Data Table using Data from Reddit](https://medium.com/swlh/dashboards-in-python-using-dash-creating-a-data-table-using-data-from-reddit-1d6c0cecb4bd) and I enjoyed the idea of [Reddit](https://www.reddit.com/) data exploration. So I thought, why not try it out with atoti?\n",
    "\n",
    "That is indeed what I did. It's a pretty short notebook used to explore the data downloaded with [atoti](https://www.atoti.io/).\n",
    "Hence I did some additional work:\n",
    "- [Natural Language Processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing) with [spaCy](https://spacy.io/) to extract [named entities](https://spacy.io/api/annotation#named-entities)\n",
    "- Real-time dashboarding with atoti\n",
    "\n",
    "To achieve real-time, I used threads to trigger the below process flow on regular intervals:\n",
    "\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/thread_task.png\" alt=\"atoti table\" width=\"800\"/>\n",
    "\n",
    "Hope you enjoy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't work, try running pip install praw in terminal\n",
    "# !pip install praw spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only has to be downloaded once. Uncomment if you haven't\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to atoti 0.4.2!\n",
      "\n",
      "By using this community edition, you agree with the license available at https://www.atoti.io/eula.\n",
      "Browse the official documentation at https://docs.atoti.io.\n",
      "Join the community at https://www.atoti.io/register.\n",
      "\n",
      "You can hide this message by setting the ATOTI_HIDE_EULA_MESSAGE environment variable to True.\n"
     ]
    }
   ],
   "source": [
    "import sched\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import atoti as tt\n",
    "import pandas as pd\n",
    "import praw\n",
    "import spacy\n",
    "from atoti.config import create_config\n",
    "\n",
    "from utils import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with spaCy and atoti  \n",
    "\n",
    "I'm going to use spaCy to perform some NLP to extract different named entities from the posting.  \n",
    "To do so, I have found a nice [reference](https://realpython.com/natural-language-processing-spacy-python/) article. Let's start by loading the [starter model](https://spacy.io/models) from spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP - Extracting named entities with spaCy\n",
    "\n",
    "Before going into the implementation of the NLP, I'm creating a `entities.csv` to which I will write the entities’ attributes once I extracted them from the post.  \n",
    "This is in preparation for real-time data loading into atoti datacube where I could leverage on the `watch` feature of `session.read_csv` to trigger the loading on update of the file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_file_name = \"entities.csv\"\n",
    "entity_file = open(entity_file_name, \"w\", encoding=\"utf-8\")\n",
    "entity_file.write(\"id|category|text|text count\\n\")\n",
    "entity_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spaCy, I am going to extract the following [named entities](https://spacy.io/api/annotation#named-entities) from each Reddit posting:  \n",
    "- Organizations\n",
    "- Person\n",
    "- Geographical Locations\n",
    "- Events\n",
    "- Product\n",
    "- NORP (Nationalities/religious/political groups)\n",
    "\n",
    "In the `get_post_entity` function below, I create a processed [Doc](https://spacy.io/api/doc) object that gives a sequence of [tokens](https://spacy.io/api/token) for each post. Using the tokens, I can identify the entity with its label.  \n",
    "\n",
    "By using `Counter(list).items()`, I will be able to get a list containing all the terms related to the mentioned entity type and the number of time it appeared. \n",
    "Note that you could also get _n_ number of _most common_ items from the list using [Counter](https://docs.python.org/2/library/collections.html). However, I chose to get all items as I could easily use atoti's filter to achieve the same result. \n",
    "\n",
    "The list of entities with its post's id is output to the `entities.csv` created above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tuples to list of entity with its attributes\n",
    "def get_entity_list(id, category, item_list):\n",
    "    # perform data clean up such as trim and removing line break and double-quotes\n",
    "    return list(\n",
    "        map(\n",
    "            lambda a: \"|\".join(\n",
    "                '\"{}\"'.format(str(e).strip().replace(\"\\n\", \" \").replace('\"', '\"\"'))\n",
    "                for e in ((id, category,) + a)\n",
    "            ),\n",
    "            list(item_list),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# for each posting/title, we extract the entities\n",
    "def get_post_entity(id, text):\n",
    "\n",
    "    post_entity = []\n",
    "    org_list = []\n",
    "    person_list = []\n",
    "    gpe_list = []\n",
    "    event_list = []\n",
    "    product_list = []\n",
    "    norp_list = []\n",
    "\n",
    "    nlp_text = nlp(text)\n",
    "    # ent is a Span object with various attributes\n",
    "    for ent in nlp_text.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            org_list.append(ent.text)\n",
    "        elif ent.label_ == \"PERSON\":\n",
    "            person_list.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            gpe_list.append(ent.text)\n",
    "        elif ent.label_ == \"EVENT\":\n",
    "            event_list.append(ent.text)\n",
    "        elif ent.label_ == \"PRODUCT\":\n",
    "            product_list.append(ent.text)\n",
    "        elif ent.label_ == \"NORP\":\n",
    "            product_list.append(ent.text)\n",
    "\n",
    "    org = Counter(org_list).items()\n",
    "    post_entity = get_entity_list(id, \"Organization\", org)\n",
    "\n",
    "    person = Counter(person_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Person\", person)\n",
    "\n",
    "    gpe = Counter(gpe_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Geographical Location\", gpe)\n",
    "\n",
    "    event = Counter(event_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Event\", event)\n",
    "\n",
    "    products = Counter(product_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Product\", products)\n",
    "    \n",
    "    norp = Counter(norp_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"NORP\", norp)\n",
    "\n",
    "    if len(post_entity) > 0:\n",
    "        entity_file.writelines(\"%s\\n\" % x for x in post_entity)\n",
    "\n",
    "    return post_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP - Preprocessing with spaCy\n",
    "\n",
    "Through preprocessing, I normalized the text:\n",
    "- lowercase\n",
    "- remove [stop words](https://en.wikipedia.org/wiki/Stop_words) (words that doesn't add much meaning to the sentence) and punctuation symbols\n",
    "- [lemmatizes](https://en.wikipedia.org/wiki/Lemmatisation) each token.\n",
    "\n",
    "This way, the named entities extracted can easily be grouped to compute the number of times it is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/natural-language-processing-spacy-python/\n",
    "def is_token_allowed(token):\n",
    "    if not token or not token.string.strip() or token.is_stop or token.is_punct:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "def normalize(category, id, post):\n",
    "    nlp_text = nlp(post)\n",
    "\n",
    "    complete_filtered_tokens = [\n",
    "        preprocess_token(token) for token in nlp_text if is_token_allowed(token)\n",
    "    ]\n",
    "\n",
    "    lemmatized_sentence = \" \".join(complete_filtered_tokens)\n",
    "\n",
    "    if category == \"title\":\n",
    "        get_post_entity(id, lemmatized_sentence)\n",
    "\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the post invokes `get_post_entity`, triggering the extraction of named entities into csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW to connect to Reddit\n",
    "\n",
    "[PRAW](https://praw.readthedocs.io/en/latest/) is the Python Reddit API Wrapper that we used to download the submissions from the _subreddits_.\n",
    "I'm not going into the details of the data scraping.  \n",
    "You can check out the original article on how you can get the [API key](https://medium.com/swlh/dashboards-in-python-using-dash-creating-a-data-table-using-data-from-reddit-1d6c0cecb4bd#08db).  \n",
    "I've excluded the config file from the source :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reddit connection\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config.cid, client_secret=config.csec, user_agent=config.ua\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do more analysis, I created a function to get the latest postings. This function will allow me to configure the number of postings to download, and from which subreddit to download the postings from.\n",
    "\n",
    "Unlike the original article, I decided to format `created` into _datetime_ and splitted it into `created_date` and `created_time` as I figured this would allow me to have consolidated view on date level or drill down to the time level.\n",
    "\n",
    "Referencing the [PRAW documentation](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html#submission) for the submission attributes, I added _id_ and _author_ to the list as well. Important to note is that for real-time, I will be polling the latest 100 postings at a 30 seconds intervals. Potentially there will be duplicated postings across the pollings. Storing the _id_ allows me to **leverage on atoti's capability to update existing posts instead of inserting duplicated postings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_posts(subreddit, count):\n",
    "    # list for df conversion\n",
    "    _posts = []\n",
    "    # return 100 new posts from wallstreetbets\n",
    "    new_bets = reddit.subreddit(subreddit).new(limit=count)\n",
    "    # return the important attributes\n",
    "    for post in new_bets:\n",
    "        # normalize the post and perform NLP named entities extraction\n",
    "        lemmatized_post = normalize(\"post\", post.id, post.selftext)\n",
    "        lemmatized_title = normalize(\"title\", post.id, post.title)\n",
    "        _posts.append(\n",
    "            [\n",
    "                post.id,\n",
    "                post.author,\n",
    "                post.title,\n",
    "                post.score,\n",
    "                post.num_comments,\n",
    "                post.selftext,\n",
    "                lemmatized_post,\n",
    "                lemmatized_title,\n",
    "                post.created,\n",
    "                post.pinned,\n",
    "                post.total_awards_received,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # create a dataframe\n",
    "    _posts = pd.DataFrame(\n",
    "        _posts,\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"author\",\n",
    "            \"title\",\n",
    "            \"score\",\n",
    "            \"comments\",\n",
    "            \"post\",\n",
    "            \"lemmatized post\",\n",
    "            \"lemmatized title\",\n",
    "            \"created\",\n",
    "            \"pinned\",\n",
    "            \"total awards\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    _posts[\"created\"] = pd.to_datetime(_posts[\"created\"], unit=\"s\")\n",
    "    _posts[\"created date\"] = pd.to_datetime(_posts[\"created\"], unit=\"s\").dt.date\n",
    "    _posts[\"created time\"] = pd.to_datetime(_posts[\"created\"], unit=\"s\").dt.time\n",
    "    _posts[\"subreddit\"] = subreddit\n",
    "\n",
    "    return _posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kickstart, I test downloaded the newest 500 post from the subreddit _wallstreetbets_.  \n",
    "This data will be loaded into atoti's **in-memory datacube** later on. This means that the **amount of data that can be loaded is limited only by the hosting machine's capacity**. You could adjust the number of postings you want to look at according to your machine's memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = get_latest_posts(\"wallstreetbets\", 500)\n",
    "entity_file.flush()\n",
    "\n",
    "# check the number of posts retrieved\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adapted the below code from the original article and used the Reddit's title instead of post for NLP. Mainly because the Subreddits that I chose has more text in the title than the post.  \n",
    "The function uses Pandas dataframe to compute some simple statistics about the text in the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_features(_posts):\n",
    "    # copy the dataframe\n",
    "    df = _posts.copy()\n",
    "    # count words in post\n",
    "    df[\"words\"] = df[\"title\"].apply(lambda x: len(x.split()))\n",
    "    # count characters in post\n",
    "    df[\"chars\"] = df[\"title\"].apply(lambda x: len(x.replace(\" \", \"\")))\n",
    "    # calculate word density\n",
    "    df[\"word density\"] = (df[\"words\"] / (df[\"chars\"] + 1)).round(3)\n",
    "    # count unique words\n",
    "    df[\"unique words\"] = df[\"title\"].apply(lambda x: len(set(w for w in x.split())))\n",
    "    # percent of unique words\n",
    "    df[\"unique density\"] = (df[\"unique words\"] / df[\"words\"]).round(3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the function above on the posts that I have downloaded, we will get a dataset with similar structure to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>post</th>\n",
       "      <th>lemmatized post</th>\n",
       "      <th>lemmatized title</th>\n",
       "      <th>created</th>\n",
       "      <th>pinned</th>\n",
       "      <th>total awards</th>\n",
       "      <th>created date</th>\n",
       "      <th>created time</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>word density</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idtfsn</td>\n",
       "      <td>PurpleChakras</td>\n",
       "      <td>What’s with these puts??</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>So I’m on RH, checking out JPM...why are 84 pu...</td>\n",
       "      <td>rh check jpm 84 put 8/28 8,300 help autist und...</td>\n",
       "      <td>put</td>\n",
       "      <td>2020-08-21 17:14:11</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>17:14:11</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.182</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idta0j</td>\n",
       "      <td>Bellweirboy</td>\n",
       "      <td>Love you guys - more than you will ever know -...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td># \"This Is NOT Normal\": A Stunned Morgan Stanl...</td>\n",
       "      <td>normal stunned morgan stanley say volumes way ...</td>\n",
       "      <td>love guy know warning market die</td>\n",
       "      <td>2020-08-21 16:59:38</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>16:59:38</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>18</td>\n",
       "      <td>69</td>\n",
       "      <td>0.257</td>\n",
       "      <td>16</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>idt9yg</td>\n",
       "      <td>chris24898</td>\n",
       "      <td>Ghandi</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>First they ignore you\\n\\nThen they laugh at yo...</td>\n",
       "      <td>ignore laugh laugh hahahahahahaha stupid burst...</td>\n",
       "      <td>ghandi</td>\n",
       "      <td>2020-08-21 16:59:27</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>16:59:27</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         author                                              title  \\\n",
       "0  idtfsn  PurpleChakras                           What’s with these puts??   \n",
       "1  idta0j    Bellweirboy  Love you guys - more than you will ever know -...   \n",
       "2  idt9yg     chris24898                                             Ghandi   \n",
       "\n",
       "   score  comments                                               post  \\\n",
       "0      0        11  So I’m on RH, checking out JPM...why are 84 pu...   \n",
       "1      4         8  # \"This Is NOT Normal\": A Stunned Morgan Stanl...   \n",
       "2      1         2  First they ignore you\\n\\nThen they laugh at yo...   \n",
       "\n",
       "                                     lemmatized post  \\\n",
       "0  rh check jpm 84 put 8/28 8,300 help autist und...   \n",
       "1  normal stunned morgan stanley say volumes way ...   \n",
       "2  ignore laugh laugh hahahahahahaha stupid burst...   \n",
       "\n",
       "                   lemmatized title             created  pinned  total awards  \\\n",
       "0                               put 2020-08-21 17:14:11   False             0   \n",
       "1  love guy know warning market die 2020-08-21 16:59:38   False             0   \n",
       "2                            ghandi 2020-08-21 16:59:27   False             0   \n",
       "\n",
       "  created date created time       subreddit  words  chars  word density  \\\n",
       "0   2020-08-21     17:14:11  wallstreetbets      4     21         0.182   \n",
       "1   2020-08-21     16:59:38  wallstreetbets     18     69         0.257   \n",
       "2   2020-08-21     16:59:27  wallstreetbets      1      6         0.143   \n",
       "\n",
       "   unique words  unique density  \n",
       "0             4           1.000  \n",
       "1            16           0.889  \n",
       "2             1           1.000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_nlp_features(posts)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## atoti cube creation for NLP analysis\n",
    "\n",
    "Finally we can get started with atoti. Let's create a session with the [sampling_mode](https://docs.atoti.io/0.4.2/lib/atoti.html#module-atoti.sampling) to set full, so that I can see all the data once it is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_config(metadata_db=\"./metadata.db\", sampling_mode=tt.sampling.FULL)\n",
    "session = tt.create_session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### atoti datastores\n",
    "\n",
    "I will create a `Reddit Posts` store for the data as per the original article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>post</th>\n",
       "      <th>lemmatized post</th>\n",
       "      <th>lemmatized title</th>\n",
       "      <th>created</th>\n",
       "      <th>pinned</th>\n",
       "      <th>total awards</th>\n",
       "      <th>created date</th>\n",
       "      <th>created time</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>word density</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique density</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idtfsn</th>\n",
       "      <td>PurpleChakras</td>\n",
       "      <td>What’s with these puts??</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>So I’m on RH, checking out JPM...why are 84 pu...</td>\n",
       "      <td>rh check jpm 84 put 8/28 8,300 help autist und...</td>\n",
       "      <td>put</td>\n",
       "      <td>2020-08-21T17:14:11</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>17:14:11</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0.182</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idta0j</th>\n",
       "      <td>Bellweirboy</td>\n",
       "      <td>Love you guys - more than you will ever know -...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td># \"This Is NOT Normal\": A Stunned Morgan Stanl...</td>\n",
       "      <td>normal stunned morgan stanley say volumes way ...</td>\n",
       "      <td>love guy know warning market die</td>\n",
       "      <td>2020-08-21T16:59:38</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>16:59:38</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>18</td>\n",
       "      <td>69</td>\n",
       "      <td>0.257</td>\n",
       "      <td>16</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idt9yg</th>\n",
       "      <td>chris24898</td>\n",
       "      <td>Ghandi</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>First they ignore you\\r\\n\\r\\nThen they laugh a...</td>\n",
       "      <td>ignore laugh laugh hahahahahahaha stupid burst...</td>\n",
       "      <td>ghandi</td>\n",
       "      <td>2020-08-21T16:59:27</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>16:59:27</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title  \\\n",
       "id                                                                         \n",
       "idtfsn  PurpleChakras                           What’s with these puts??   \n",
       "idta0j    Bellweirboy  Love you guys - more than you will ever know -...   \n",
       "idt9yg     chris24898                                             Ghandi   \n",
       "\n",
       "        score  comments                                               post  \\\n",
       "id                                                                           \n",
       "idtfsn      0        11  So I’m on RH, checking out JPM...why are 84 pu...   \n",
       "idta0j      4         8  # \"This Is NOT Normal\": A Stunned Morgan Stanl...   \n",
       "idt9yg      1         2  First they ignore you\\r\\n\\r\\nThen they laugh a...   \n",
       "\n",
       "                                          lemmatized post  \\\n",
       "id                                                          \n",
       "idtfsn  rh check jpm 84 put 8/28 8,300 help autist und...   \n",
       "idta0j  normal stunned morgan stanley say volumes way ...   \n",
       "idt9yg  ignore laugh laugh hahahahahahaha stupid burst...   \n",
       "\n",
       "                        lemmatized title              created  pinned  \\\n",
       "id                                                                      \n",
       "idtfsn                               put  2020-08-21T17:14:11   False   \n",
       "idta0j  love guy know warning market die  2020-08-21T16:59:38   False   \n",
       "idt9yg                            ghandi  2020-08-21T16:59:27   False   \n",
       "\n",
       "        total awards created date created time       subreddit  words  chars  \\\n",
       "id                                                                             \n",
       "idtfsn             0   2020-08-21     17:14:11  wallstreetbets      4     21   \n",
       "idta0j             0   2020-08-21     16:59:38  wallstreetbets     18     69   \n",
       "idt9yg             0   2020-08-21     16:59:27  wallstreetbets      1      6   \n",
       "\n",
       "        word density  unique words  unique density  \n",
       "id                                                  \n",
       "idtfsn         0.182             4           1.000  \n",
       "idta0j         0.257            16           0.889  \n",
       "idt9yg         0.143             1           1.000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_store = session.read_pandas(df, keys=[\"id\"], store_name=\"Reddit Posts\")\n",
    "reddit_store.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my experiment with spaCy, I create a `Entities` store to store the named entities.  \n",
    "Below, I set `watch=True` for the entities.csv such that any update to the file will trigger an upload to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_store = session.read_csv(\n",
    "    entity_file_name,\n",
    "    keys=[\"id\", \"category\", \"text\"],\n",
    "    store_name=\"Entities\",\n",
    "    watch=True,\n",
    "    sep=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idt67q</th>\n",
       "      <th>Organization</th>\n",
       "      <th>usd</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idt4fa</th>\n",
       "      <th>Person</th>\n",
       "      <th>diversify noob</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idse2g</th>\n",
       "      <th>Organization</th>\n",
       "      <th>reuters</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids5sf</th>\n",
       "      <th>Organization</th>\n",
       "      <th>tsla</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ids2nl</th>\n",
       "      <th>Person</th>\n",
       "      <th>covid vaccine</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    text count\n",
       "id     category     text                      \n",
       "idt67q Organization usd                      1\n",
       "idt4fa Person       diversify noob           1\n",
       "idse2g Organization reuters                  1\n",
       "ids5sf Organization tsla                     1\n",
       "ids2nl Person       covid vaccine            1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_store.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I join the `Entities` store to `Reddit Posts` which I decided to use as the base store.  \n",
    "Note that atoti automatically joins columns with the same name across both stores. You could define your own [mapping](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=join#atoti.store.Store.join) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_store.join(entity_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### atoti cube creation - No measures\n",
    "\n",
    "By default, atoti will automatically create hierarchies and measures from the datastore based on the data type of the columns.  \n",
    "In this case, I prefer to manually create my measures, hence I set the [mode](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=create_cube#atoti.session.Session.create_cube) of creation to `no_measures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = session.create_cube(reddit_store, \"Reddit\", mode=\"no_measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"426px\" height=\"756px\" viewBox=\"0.00 0.00 426.00 756.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.0 1.0) rotate(0) translate(4 752)\">\n",
       "<title>Reddit schema</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-752 422,-752 422,4 -4,4\"/>\n",
       "<!-- Reddit Posts -->\n",
       "<g id=\"node1\" class=\"node\"><title>Reddit Posts</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-707 8,-744 168,-744 168,-707 8,-707\"/>\n",
       "<text text-anchor=\"start\" x=\"49\" y=\"-722.8\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Reddit Posts</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-670 8,-707 168,-707 168,-670 8,-670\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-685.8\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"44\" y=\"-685.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">  id </text>\n",
       "<text text-anchor=\"start\" x=\"66\" y=\"-685.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"102\" y=\"-685.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-633 8,-670 168,-670 168,-633 8,-633\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-648.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">author </text>\n",
       "<text text-anchor=\"start\" x=\"59\" y=\"-648.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"95\" y=\"-648.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-596 8,-633 168,-633 168,-596 8,-596\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-611.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">title </text>\n",
       "<text text-anchor=\"start\" x=\"43\" y=\"-611.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"79\" y=\"-611.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-559 8,-596 168,-596 168,-559 8,-559\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-574.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">score </text>\n",
       "<text text-anchor=\"start\" x=\"54\" y=\"-574.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-574.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-522 8,-559 168,-559 168,-522 8,-522\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-537.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">comments </text>\n",
       "<text text-anchor=\"start\" x=\"81\" y=\"-537.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"107\" y=\"-537.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-485 8,-522 168,-522 168,-485 8,-485\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-500.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">post </text>\n",
       "<text text-anchor=\"start\" x=\"48\" y=\"-500.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"84\" y=\"-500.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-448 8,-485 168,-485 168,-448 8,-448\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-463.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lemmatized post </text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-463.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"150\" y=\"-463.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-411 8,-448 168,-448 168,-411 8,-411\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-426.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lemmatized title </text>\n",
       "<text text-anchor=\"start\" x=\"109\" y=\"-426.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"145\" y=\"-426.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-374 8,-411 168,-411 168,-374 8,-374\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-389.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">created </text>\n",
       "<text text-anchor=\"start\" x=\"64\" y=\"-389.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">LocalDateTime</text>\n",
       "<text text-anchor=\"start\" x=\"153\" y=\"-389.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-337 8,-374 168,-374 168,-337 8,-337\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-352.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pinned </text>\n",
       "<text text-anchor=\"start\" x=\"61\" y=\"-352.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">boolean</text>\n",
       "<text text-anchor=\"start\" x=\"107\" y=\"-352.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-300 8,-337 168,-337 168,-300 8,-300\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-315.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">total awards </text>\n",
       "<text text-anchor=\"start\" x=\"91\" y=\"-315.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"117\" y=\"-315.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-263 8,-300 168,-300 168,-263 8,-263\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-278.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">created date </text>\n",
       "<text text-anchor=\"start\" x=\"91\" y=\"-278.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">LocalDate</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-278.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-226 8,-263 168,-263 168,-226 8,-226\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-241.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">created time </text>\n",
       "<text text-anchor=\"start\" x=\"91\" y=\"-241.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"127\" y=\"-241.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-189 8,-226 168,-226 168,-189 8,-189\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-204.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">subreddit </text>\n",
       "<text text-anchor=\"start\" x=\"76\" y=\"-204.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"112\" y=\"-204.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-152 8,-189 168,-189 168,-152 8,-152\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-167.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">words </text>\n",
       "<text text-anchor=\"start\" x=\"59\" y=\"-167.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"85\" y=\"-167.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-115 8,-152 168,-152 168,-115 8,-115\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-130.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">chars </text>\n",
       "<text text-anchor=\"start\" x=\"53\" y=\"-130.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"79\" y=\"-130.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-78 8,-115 168,-115 168,-78 8,-78\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-93.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">word density </text>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-93.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">double</text>\n",
       "<text text-anchor=\"start\" x=\"136\" y=\"-93.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-41 8,-78 168,-78 168,-41 8,-41\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-56.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">unique words </text>\n",
       "<text text-anchor=\"start\" x=\"99\" y=\"-56.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-56.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-4 8,-41 168,-41 168,-4 8,-4\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">unique density </text>\n",
       "<text text-anchor=\"start\" x=\"104\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">double</text>\n",
       "<text text-anchor=\"start\" x=\"144\" y=\"-19.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- Entities -->\n",
       "<g id=\"node2\" class=\"node\"><title>Entities</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-429 263.5,-466 410.5,-466 410.5,-429 263.5,-429\"/>\n",
       "<text text-anchor=\"start\" x=\"313.5\" y=\"-444.8\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Entities</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-392 263.5,-429 410.5,-429 410.5,-392 263.5,-392\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">  id </text>\n",
       "<text text-anchor=\"start\" x=\"321.5\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"357.5\" y=\"-407.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-355 263.5,-392 410.5,-392 410.5,-355 263.5,-355\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-370.8\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-370.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">  category </text>\n",
       "<text text-anchor=\"start\" x=\"359.5\" y=\"-370.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"395.5\" y=\"-370.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-318 263.5,-355 410.5,-355 410.5,-318 263.5,-318\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-333.8\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-333.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">  text </text>\n",
       "<text text-anchor=\"start\" x=\"331.5\" y=\"-333.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"367.5\" y=\"-333.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-281 263.5,-318 410.5,-318 410.5,-281 263.5,-281\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-296.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">text count </text>\n",
       "<text text-anchor=\"start\" x=\"334.5\" y=\"-296.8\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">int</text>\n",
       "<text text-anchor=\"start\" x=\"350.5\" y=\"-296.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- Reddit Posts&#45;&gt;Entities -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>Reddit Posts-&gt;Entities</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M176.254,-374C198.473,-374 222.402,-374 244.79,-374\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.955,-377.5 254.955,-374 244.955,-370.5 244.955,-377.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-377.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id → id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the cube created. Only a `Count` measure is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Dimensions": {
        "Hierarchies": {
         "author": [
          "author"
         ],
         "category": [
          "category"
         ],
         "chars": [
          "chars"
         ],
         "comments": [
          "comments"
         ],
         "created": [
          "created"
         ],
         "created date": [
          "created date"
         ],
         "created time": [
          "created time"
         ],
         "id": [
          "id"
         ],
         "lemmatized post": [
          "lemmatized post"
         ],
         "lemmatized title": [
          "lemmatized title"
         ],
         "pinned": [
          "pinned"
         ],
         "post": [
          "post"
         ],
         "score": [
          "score"
         ],
         "subreddit": [
          "subreddit"
         ],
         "text": [
          "text"
         ],
         "text count": [
          "text count"
         ],
         "title": [
          "title"
         ],
         "total awards": [
          "total awards"
         ],
         "unique words": [
          "unique words"
         ],
         "words": [
          "words"
         ]
        }
       },
       "Measures": {
        "contributors.COUNT": {
         "formatter": null,
         "visible": true
        }
       }
      },
      "text/html": [
       "<ul>\n",
       "<li>Reddit\n",
       "  <ul>\n",
       "  <li>Dimensions\n",
       "    <ul>\n",
       "    <li>Hierarchies\n",
       "      <ul>\n",
       "      <li>author\n",
       "        <ol>\n",
       "        <li>author</li>\n",
       "        </ol>      </li>\n",
       "      <li>category\n",
       "        <ol>\n",
       "        <li>category</li>\n",
       "        </ol>      </li>\n",
       "      <li>chars\n",
       "        <ol>\n",
       "        <li>chars</li>\n",
       "        </ol>      </li>\n",
       "      <li>comments\n",
       "        <ol>\n",
       "        <li>comments</li>\n",
       "        </ol>      </li>\n",
       "      <li>created\n",
       "        <ol>\n",
       "        <li>created</li>\n",
       "        </ol>      </li>\n",
       "      <li>created date\n",
       "        <ol>\n",
       "        <li>created date</li>\n",
       "        </ol>      </li>\n",
       "      <li>created time\n",
       "        <ol>\n",
       "        <li>created time</li>\n",
       "        </ol>      </li>\n",
       "      <li>id\n",
       "        <ol>\n",
       "        <li>id</li>\n",
       "        </ol>      </li>\n",
       "      <li>lemmatized post\n",
       "        <ol>\n",
       "        <li>lemmatized post</li>\n",
       "        </ol>      </li>\n",
       "      <li>lemmatized title\n",
       "        <ol>\n",
       "        <li>lemmatized title</li>\n",
       "        </ol>      </li>\n",
       "      <li>pinned\n",
       "        <ol>\n",
       "        <li>pinned</li>\n",
       "        </ol>      </li>\n",
       "      <li>post\n",
       "        <ol>\n",
       "        <li>post</li>\n",
       "        </ol>      </li>\n",
       "      <li>score\n",
       "        <ol>\n",
       "        <li>score</li>\n",
       "        </ol>      </li>\n",
       "      <li>subreddit\n",
       "        <ol>\n",
       "        <li>subreddit</li>\n",
       "        </ol>      </li>\n",
       "      <li>text\n",
       "        <ol>\n",
       "        <li>text</li>\n",
       "        </ol>      </li>\n",
       "      <li>text count\n",
       "        <ol>\n",
       "        <li>text count</li>\n",
       "        </ol>      </li>\n",
       "      <li>title\n",
       "        <ol>\n",
       "        <li>title</li>\n",
       "        </ol>      </li>\n",
       "      <li>total awards\n",
       "        <ol>\n",
       "        <li>total awards</li>\n",
       "        </ol>      </li>\n",
       "      <li>unique words\n",
       "        <ol>\n",
       "        <li>unique words</li>\n",
       "        </ol>      </li>\n",
       "      <li>words\n",
       "        <ol>\n",
       "        <li>words</li>\n",
       "        </ol>      </li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  <li>Measures\n",
       "    <ul>\n",
       "    <li>contributors.COUNT\n",
       "      <ul>\n",
       "      <li>formatter: None</li>\n",
       "      <li>visible: True</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  </ul>\n",
       "</li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<atoti.cube.Cube at 0x1b14adf5340>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "Reddit"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = cube.measures\n",
    "l = cube.levels\n",
    "h = cube.hierarchies\n",
    "cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing hierarchies\n",
    "\n",
    "Since I am going to create my own measures, I will delete the numeric columns from my hierarchies to avoid confusion with the measures that I will be creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h[\"text count\"]\n",
    "del h[\"score\"]\n",
    "del h[\"comments\"]\n",
    "del h[\"total awards\"]\n",
    "del h[\"words\"]\n",
    "del h[\"chars\"]\n",
    "del h[\"unique words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures creation\n",
    "\n",
    "In default `auto` mode, atoti will create a `MEAN` and a `SUM` measure.  \n",
    "However, as I only require `sum` in my simple use case and in order to demonstrate how simple aggregation can be done with atoti, I created the below measures instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"Text Count\"] = tt.agg.sum(entity_store[\"text count\"])\n",
    "m[\"Score\"] = tt.agg.sum(reddit_store[\"score\"])\n",
    "m[\"Comments\"] = tt.agg.sum(reddit_store[\"comments\"])\n",
    "m[\"Total awards\"] = tt.agg.sum(reddit_store[\"total awards\"])\n",
    "m[\"Words\"] = tt.agg.sum(reddit_store[\"words\"])\n",
    "m[\"Chars\"] = tt.agg.sum(reddit_store[\"chars\"])\n",
    "m[\"Unique words\"] = tt.agg.sum(reddit_store[\"unique words\"])\n",
    "m[\"Word density\"] = tt.agg.sum(reddit_store[\"word density\"])\n",
    "m[\"Unique density\"] = tt.agg.sum(reddit_store[\"unique density\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distinct measure\n",
    "\n",
    "I use the [`count_distinct`](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=count_distinct#atoti.agg.count_distinct) function to compute the number of days the data span across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"Number days\"] = tt.agg.count_distinct(reddit_store[\"created date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cumulative measure\n",
    "\n",
    "Aggregation with the [`cumulatve`](https://docs.atoti.io/0.4.2/lib/atoti.scope.html?highlight=cumulative#atoti.scope.cumulative) scope allows me to see the trend of the posting over the `created` level. By setting the parameter `dense=True`, we can see a continuous plot of the trend even if there are no postings on days in between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"Cumulative Count\"] = tt.agg.sum(\n",
    "    m[\"contributors.COUNT\"], scope=tt.scope.cumulative(l[\"created\"], dense=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "atoti": {
     "state": {
      "name": "Posting Trends",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "mapping": {
          "horizontalSubplots": [],
          "splitBy": [
           "[Hierarchies].[subreddit].[subreddit]"
          ],
          "values": [
           "[Measures].[Cumulative Count]"
          ],
          "verticalSubplots": [],
          "xAxis": [
           "[Hierarchies].[created].[created]"
          ]
         },
         "plotly": {
          "data": {
           "commonTraceOverride": {
            "mode": "lines"
           }
          }
         },
         "type": "plotly-line-chart"
        },
        "query": {
         "mdx": "SELECT NON EMPTY Crossjoin([Hierarchies].[created].[created].Members, [Hierarchies].[subreddit].[subreddit].Members) ON ROWS, NON EMPTY [Measures].[Cumulative Count] ON COLUMNS FROM [Reddit] CELL PROPERTIES VALUE, FORMATTED_VALUE, BACK_COLOR, FORE_COLOR, FONT_FLAGS",
         "serverUrl": "",
         "updateMode": "once"
        }
       },
       "containerKey": "chart",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.atoti.v0+json": {
       "cube": "Reddit",
       "name": "Posting Trends",
       "session": "Unnamed"
      },
      "text/plain": [
       "Install and enable the atoti JupyterLab extension to see this widget."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube.visualize(\"Posting Trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## atoti Pivot table against Dash DataTable\n",
    "\n",
    "Now the moment of truth! How will atoti data table fare compared to the [Dash DataTable](https://medium.com/swlh/dashboards-in-python-using-dash-creating-a-data-table-using-data-from-reddit-1d6c0cecb4bd#be7c) in the original article? _Maybe_ Dash Datatable has evolved since the article was last published, let's just enjoy the atoti table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "atoti": {
     "height": 606,
     "state": {
      "name": "Reddit Data Table",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "tabular": {
          "cellRenderers": [],
          "columns": [
           {
            "key": "[Hierarchies].[subreddit].[subreddit]",
            "width": 90
           },
           {
            "key": "[Hierarchies].[title].[title]",
            "width": 274
           },
           {
            "key": "[Hierarchies].[post].[post]",
            "width": 233
           },
           {
            "key": "[Hierarchies].[pinned].[pinned]",
            "width": 80
           },
           {
            "key": "[Hierarchies].[created].[created]",
            "width": 120
           },
           {
            "key": "[Measures].[Chars]",
            "width": 80
           },
           {
            "key": "[Measures].[Comments (1)]",
            "width": 80
           },
           {
            "key": "[Measures].[Score (1)]",
            "width": 80
           },
           {
            "key": "[Measures].[Total awards]",
            "width": 80
           },
           {
            "key": "[Measures].[Unique density]",
            "width": 80
           },
           {
            "key": "[Measures].[Unique words]",
            "width": 80
           },
           {
            "key": "[Measures].[Word density]",
            "width": 80
           }
          ],
          "columnsGroups": [
           {
            "captionProducer": "firstColumn",
            "cellFactory": "kpi-status",
            "selector": "kpi-status"
           },
           {
            "captionProducer": "firstColumn",
            "cellFactory": "lookup",
            "selector": "lookup"
           },
           {
            "captionProducer": "expiry",
            "cellFactory": "expiry",
            "selector": "kpi-expiry"
           }
          ],
          "defaultOptions": {},
          "expansion": {
           "automaticExpansion": true
          },
          "hideAddButton": true,
          "pinnedHeaderSelector": "member",
          "sortingMode": "breaking",
          "statisticsShown": true
         }
        },
        "contextValues": {},
        "mdx": "WITH  Member [Measures].[Comments (1)] AS [Measures].[Comments], CAPTION = [Measures].[Comments].MEMBER_CAPTION, FORMAT_STRING = \"#,###\"    Member [Measures].[Score (1)] AS [Measures].[Score], CAPTION = [Measures].[Score].MEMBER_CAPTION, FORMAT_STRING = \"#,###\"  SELECT NON EMPTY {[Measures].[Chars], [Measures].[Comments (1)], [Measures].[Score (1)], [Measures].[Total awards], [Measures].[Unique density], [Measures].[Unique words], [Measures].[Word density]} ON COLUMNS, NON EMPTY Order(Crossjoin([Hierarchies].[subreddit].[subreddit].Members, [Hierarchies].[title].[title].Members, [Hierarchies].[post].[post].Members, [Hierarchies].[pinned].[pinned].Members, [Hierarchies].[created].[created].Members), [Measures].[Chars], BDESC) ON ROWS FROM (SELECT [Hierarchies].[subreddit].[ALL].[AllMember].[wallstreetbets] ON COLUMNS FROM [Reddit]) CELL PROPERTIES BACK_COLOR, FONT_FLAGS, FORE_COLOR, FORMATTED_VALUE, VALUE",
        "ranges": {
         "column": {
          "chunkSize": 50,
          "thresholdPercentage": 0.2
         },
         "row": {
          "chunkSize": 2000,
          "thresholdPercentage": 0.1
         }
        },
        "serverUrl": "",
        "updateMode": "once"
       },
       "containerKey": "pivot-table",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.atoti.v0+json": {
       "cube": "Reddit",
       "name": "Reddit Data Table",
       "session": "Unnamed"
      },
      "text/plain": [
       "Install and enable the atoti JupyterLab extension to see this widget."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube.visualize(\"Reddit Data Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can easily adjust the height of the table or the width of the columns. With the atoti Editor, I can easily:\n",
    "- add or remove the levels and measures\n",
    "- filter the data\n",
    "- sort the measures\n",
    "- format the columns\n",
    "\n",
    "Refer to the GIF below to see how these could be done.\n",
    "\n",
    "##### Creating table\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/creating_table.gif\" alt=\"atoti table\" width=\"1080\"/>\n",
    "\n",
    "##### Table adjustment\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/table_adjustment.gif\" alt=\"atoti table adjustment\" width=\"1080\"/>\n",
    "\n",
    "##### Cell formatting and misc\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/table_formatting.gif\" alt=\"atoti cell formatting\" width=\"1080\"/>\n",
    "\n",
    "\n",
    "I can toggle the table from a tabular table to pivot table. Pivot table allows me to drill down on any hierarchies. The measures are computed on the fly as the level changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "atoti": {
     "height": 423,
     "state": {
      "name": "",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "tabular": {
          "addButtonFilter": "numeric",
          "cellRenderers": [
           "tree-layout"
          ],
          "columnsGroups": [
           {
            "captionProducer": "firstColumn",
            "cellFactory": "kpi-status",
            "selector": "kpi-status"
           },
           {
            "captionProducer": "firstColumn",
            "cellFactory": "lookup",
            "selector": "lookup"
           },
           {
            "captionProducer": "expiry",
            "cellFactory": "expiry",
            "selector": "kpi-expiry"
           },
           {
            "captionProducer": "columnMerge",
            "cellFactory": {
             "args": {},
             "key": "treeCells"
            },
            "selector": "member"
           }
          ],
          "defaultOptions": {},
          "expansion": {
           "automaticExpansion": true
          },
          "hideAddButton": true,
          "pinnedHeaderSelector": "member",
          "sortingMode": "non-breaking",
          "statisticsShown": true
         }
        },
        "contextValues": {},
        "mdx": "SELECT NON EMPTY {[Measures].[Total awards], [Measures].[Comments], [Measures].[contributors.COUNT]} ON COLUMNS, NON EMPTY Crossjoin(Hierarchize(DrilldownLevel([Hierarchies].[subreddit].[ALL].[AllMember])), Hierarchize(DrilldownLevel([Hierarchies].[category].[ALL].[AllMember])), Hierarchize(DrilldownLevel([Hierarchies].[created date].[ALL].[AllMember]))) ON ROWS FROM [Reddit]",
        "ranges": {
         "column": {
          "chunkSize": 50,
          "thresholdPercentage": 0.2
         },
         "row": {
          "chunkSize": 2000,
          "thresholdPercentage": 0.1
         }
        },
        "serverUrl": "",
        "updateMode": "once"
       },
       "containerKey": "pivot-table",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.atoti.v0+json": {
       "cube": "Reddit",
       "name": null,
       "session": "Unnamed"
      },
      "text/plain": [
       "Install and enable the atoti JupyterLab extension to see this widget."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time dashboarding with atoti\n",
    "\n",
    "At the moment, I have only loaded data from the subreddit _wallstreetbets_. In the next few cell, I have created functions that allow me to poll the latest 200 postings from some other subreddit groups:\n",
    "- news\n",
    "- worldnews\n",
    "- politics\n",
    "- technology\n",
    "- wallstreetbets\n",
    "\n",
    "Let's switch over to a dashboard that I have prepared in advance by accessing the url from the next cell. After loading the dashboard, trigger the rest of the cells to start the data polling. You can then go back to the dashboard to see it getting refreshed each time new data is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:58673/#/dashboard/c7c'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.url + \"/#/dashboard/c7c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used [`store.load_pandas`](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=load_pandas#atoti.store.Store.load_pandas) to load the Pandas dataframe incrementally into the `Reddit Posts` store. For the `Entities` store, the csv file is on `watch` mode, hence any update will trigger data to be uploaded into the datastore. Based on the stores' keys, existing data will be updated and new data will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_subreddit(subreddit):\n",
    "    posts_news = get_latest_posts(subreddit, 200)\n",
    "    df_news = get_nlp_features(posts_news)\n",
    "\n",
    "    reddit_store.load_pandas(df_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demo purpose, I have only set the polling to iterate 10 times with intervals of 30s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "\n",
    "should_publish = threading.Event()\n",
    "update_queue = queue.Queue()\n",
    "\n",
    "\n",
    "def start_publisher():\n",
    "    # Do 10 iterations with 30s interval\n",
    "    starttime = time.time()\n",
    "    print(\"Start polling\", starttime)\n",
    "    poll_iteration = 1\n",
    "\n",
    "    while poll_iteration <= 10:\n",
    "        print(\"\\rpublishing update \", end=\"\")\n",
    "        update_queue.put((poll_iteration))\n",
    "        poll_iteration += 1\n",
    "        time.sleep(30)\n",
    "        print(\"\\rawaiting for publishing update\", end=\"\")\n",
    "        should_publish.wait()\n",
    "        update_queue.join()\n",
    "\n",
    "\n",
    "def start_update_listener():\n",
    "    while True:\n",
    "        poll_iteration = update_queue.get()\n",
    "        print(poll_iteration, \">>> Start polling from subreddit... \")\n",
    "        # poll from subreddit\n",
    "        load_new_subreddit(\"news\")\n",
    "        print(poll_iteration, \"........ Loaded news \")\n",
    "        \n",
    "        load_new_subreddit(\"worldnews\")\n",
    "        print(poll_iteration, \"........ Loaded worldnews \")\n",
    "        \n",
    "        load_new_subreddit(\"politics\")\n",
    "        print(poll_iteration, \"........ Loaded politics \")\n",
    "        \n",
    "        load_new_subreddit(\"technology\")\n",
    "        print(poll_iteration, \"........ Loaded technology \")\n",
    "        \n",
    "        load_new_subreddit(\"wallstreetbets\")\n",
    "        print(poll_iteration, \"........ Loaded wallstreetbets \")\n",
    "        entity_file.flush()\n",
    "        print(poll_iteration, \">>> End polling from subreddit... \")\n",
    "        update_queue.task_done()\n",
    "\n",
    "\n",
    "listener_thread = threading.Thread(target=start_update_listener, daemon=True)\n",
    "publisher_thread = threading.Thread(target=start_publisher, daemon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start polling 1598002153.362756\n",
      "publishing update 1 >>> Start polling from subreddit... \n",
      "1 ........ Loaded news \n",
      "1 ........ Loaded worldnews \n",
      "1 ........ Loaded politics \n",
      "1 ........ Loaded technology \n",
      "awaiting for publishing update1 ........ Loaded wallstreetbets \n",
      "1 >>> End polling from subreddit... \n",
      "publishing update 2 >>> Start polling from subreddit... \n",
      "2 ........ Loaded news \n",
      "2 ........ Loaded worldnews \n",
      "2 ........ Loaded politics \n",
      "2 ........ Loaded technology \n",
      "2 ........ Loaded wallstreetbets \n",
      "2 >>> End polling from subreddit... \n",
      "publishing update 3 >>> Start polling from subreddit... \n",
      "3 ........ Loaded news \n",
      "3 ........ Loaded worldnews \n",
      "3 ........ Loaded politics \n",
      "3 ........ Loaded technology \n",
      "awaiting for publishing update3 ........ Loaded wallstreetbets \n",
      "3 >>> End polling from subreddit... \n",
      "publishing update 4 >>> Start polling from subreddit... \n",
      "4 ........ Loaded news \n",
      "4 ........ Loaded worldnews \n",
      "4 ........ Loaded politics \n",
      "4 ........ Loaded technology \n",
      "awaiting for publishing update4 ........ Loaded wallstreetbets \n",
      "4 >>> End polling from subreddit... \n",
      "publishing update 5 >>> Start polling from subreddit... \n",
      "5 ........ Loaded news \n",
      "5 ........ Loaded worldnews \n",
      "5 ........ Loaded politics \n",
      "5 ........ Loaded technology \n",
      "awaiting for publishing update5 ........ Loaded wallstreetbets \n",
      "5 >>> End polling from subreddit... \n",
      "publishing update 6 >>> Start polling from subreddit... \n",
      "6 ........ Loaded news \n",
      "6 ........ Loaded worldnews \n",
      "6 ........ Loaded politics \n",
      "6 ........ Loaded technology \n",
      "awaiting for publishing update6 ........ Loaded wallstreetbets \n",
      "6 >>> End polling from subreddit... \n",
      "publishing update 7 >>> Start polling from subreddit... \n",
      "7 ........ Loaded news \n",
      "7 ........ Loaded worldnews \n",
      "7 ........ Loaded politics \n",
      "7 ........ Loaded technology \n",
      "awaiting for publishing update7 ........ Loaded wallstreetbets \n",
      "7 >>> End polling from subreddit... \n",
      "publishing update 8 >>> Start polling from subreddit... \n",
      "8 ........ Loaded news \n",
      "8 ........ Loaded worldnews \n",
      "8 ........ Loaded politics \n",
      "awaiting for publishing update8 ........ Loaded technology \n",
      "8 ........ Loaded wallstreetbets \n",
      "8 >>> End polling from subreddit... \n",
      "publishing update 9 >>> Start polling from subreddit... \n",
      "9 ........ Loaded news \n",
      "9 ........ Loaded worldnews \n",
      "9 ........ Loaded politics \n",
      "9 ........ Loaded technology \n",
      "awaiting for publishing update9 ........ Loaded wallstreetbets \n",
      "9 >>> End polling from subreddit... \n",
      "publishing update 10 >>> Start polling from subreddit... \n",
      "10 ........ Loaded news \n",
      "10 ........ Loaded worldnews \n",
      "10 ........ Loaded politics \n",
      "10 ........ Loaded technology \n",
      "awaiting for publishing update10 ........ Loaded wallstreetbets \n",
      "10 >>> End polling from subreddit... \n"
     ]
    }
   ],
   "source": [
    "publisher_thread.start()\n",
    "listener_thread.start()\n",
    "# start publishing\n",
    "should_publish.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to stop the data polling before the 20 iterations end, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pause publishing\n",
    "should_publish.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard Page - Post Frequency \n",
    "\n",
    "This is how the real-time dashboard will look like when the scheduler is running:\n",
    "\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/real-time_dashboarding.gif\" alt=\"atoti table\" width=\"1080\"/>\n",
    "\n",
    "Using the Subreddit's submission id as a unique key, atoti is able to update an existing posting and insert new ones.  \n",
    "You can see the frequency of posting and trend from each Subreddit.  \n",
    "From the tree map, you can also see which author's posting generates the most comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard Page - NLP Stats\n",
    "\n",
    "Go to second tab of the dashboard to play with the NLP Statistics using quick filters:\n",
    "\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/NLP_Stats.gif\" alt=\"atoti table\" width=\"1080\"/>\n",
    "\n",
    "If there are too many values to choose, you can also use the \"Label contains...\" filter option to do a _partial label_ search. Try selecting Trump and Biden to see the number of postings related to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dashboard Page - Author Analysis\n",
    "\n",
    "Go to third tab of the dashboard for interactive analysis of authors:\n",
    "\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/author_analysis.gif\" alt=\"atoti table\" width=\"1080\"/>\n",
    "\n",
    "Perform a drillthrough to see the underlying data behind the selected cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun exploring the dashboard! Do play around with some of the ootb widgets such as \"Quick Page Filter\" that allow you to easily filter the page from the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
