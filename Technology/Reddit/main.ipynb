{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2020-07-23T05:31:14.898946Z",
     "shell.execute_reply": "2020-07-23T05:31:14.897998Z",
     "shell.execute_reply.started": "2020-07-23T05:31:13.964682Z"
    }
   },
   "source": [
    "# NLP exploration with Reddit data\n",
    "\n",
    "I come across this old article about [Creating a Data Table using Data from Reddit](https://medium.com/swlh/dashboards-in-python-using-dash-creating-a-data-table-using-data-from-reddit-1d6c0cecb4bd) and I enjoyed the idea of Reddit data exploration. So I thought, why not try it out with atoti?\n",
    "\n",
    "That is indeed what I did. It's a pretty short notebook used to explore the data downloaded with [atoti](https://www.atoti.io/).\n",
    "Hence I did some additional work:\n",
    "- NLP with spaCy to extract [named entities](https://spacy.io/api/annotation#named-entities)\n",
    "- Real-time dashboarding with atoti\n",
    "\n",
    "Hope you enjoy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't work, try running pip install praw in terminal\n",
    "# !pip install praw spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only has to be downloaded once. Uncomment if you haven't\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to atoti 0.4.2!\n",
      "\n",
      "By using this community edition, you agree with the license available at https://www.atoti.io/eula.\n",
      "Browse the official documentation at https://docs.atoti.io.\n",
      "Join the community at https://www.atoti.io/register.\n",
      "\n",
      "You can hide this message by setting the ATOTI_HIDE_EULA_MESSAGE environment variable to True.\n"
     ]
    }
   ],
   "source": [
    "import sched\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import atoti as tt\n",
    "import pandas as pd\n",
    "import praw\n",
    "import spacy\n",
    "from atoti.config import create_config\n",
    "\n",
    "from utils import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with spaCy and atoti  \n",
    "\n",
    "I'm going to use [spaCy](https://spacy.io/) to perform some [Natural Language Processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing) to extract different named entities from the posting.  \n",
    "To do so, I have found a nice [reference](https://realpython.com/natural-language-processing-spacy-python/) article. Let's start by downloading the [starter model](https://spacy.io/models) from spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP - Extracting named entities with spaCy\n",
    "\n",
    "Before going into the implementation of the NLP, I'm creating a `entities.csv` to store the entities' attributes.  \n",
    "This is in preparation for real-time data loading into atoti datacube where I could leverage on the `watch` feature of `session.read_csv` to trigger data loading into the cube on update of the file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_file_name = \"entities.csv\"\n",
    "entity_file = open(entity_file_name, \"w\", encoding=\"utf-8\")\n",
    "entity_file.write(\"id|category|text|text count\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spaCy, I am going to extract the following [named entities](https://spacy.io/api/annotation#named-entities) from each Reddit posting:  \n",
    "- Organizations\n",
    "- Person\n",
    "- Geographical Locations\n",
    "- Events\n",
    "\n",
    "In the `get_post_entity` function below, I create a processed [Doc](https://spacy.io/api/doc) object that gives a sequence of [tokens](https://spacy.io/api/token) for each post. Using the tokens, I can identify the entity with its label.  \n",
    "\n",
    "By using `Counter(list).items()`, I will be able to get a list containing all the terms related to the mentioned entity type and the number of time it appeared. \n",
    "Note that you could also get _n_ number of _most common_ items from the list using [Counter](https://docs.python.org/2/library/collections.html). However, I chose to get all items as I could easily use atoti's filter to achieve the same result. \n",
    "\n",
    "The list of entities with its post's id is output to the `entities.csv` created above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of tuples to list of entity with its attributes\n",
    "def get_entity_list(id, category, item_list):\n",
    "    # perform data clean up such as trim and removing line break and double-quotes\n",
    "    return list(\n",
    "        map(\n",
    "            lambda a: \"|\".join(\n",
    "                '\"{}\"'.format(str(e).strip().replace(\"\\n\", \" \").replace('\"', \"\"))\n",
    "                for e in ((id, category,) + a)\n",
    "            ),\n",
    "            list(item_list),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# for each posting/title, we extract the entities\n",
    "def get_post_entity(id, nlp_text):\n",
    "\n",
    "    post_entity = []\n",
    "    org_list = []\n",
    "    person_list = []\n",
    "    gpe_list = []\n",
    "    event_list = []\n",
    "\n",
    "    # ent is a Span object with various attributes\n",
    "    for ent in nlp_text.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            org_list.append(ent.text)\n",
    "        elif ent.label_ == \"PERSON\":\n",
    "            person_list.append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            gpe_list.append(ent.text)\n",
    "        elif ent.label_ == \"EVENT\":\n",
    "            event_list.append(ent.text)\n",
    "\n",
    "    org = Counter(org_list).items()\n",
    "    post_entity = get_entity_list(id, \"Organization\", org)\n",
    "\n",
    "    person = Counter(person_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Person\", person)\n",
    "\n",
    "    gpe = Counter(gpe_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Geographical Location\", gpe)\n",
    "\n",
    "    event = Counter(event_list).items()\n",
    "    post_entity = post_entity + get_entity_list(id, \"Event\", event)\n",
    "\n",
    "    entity_file.writelines(\"%s\\n\" % x for x in post_entity)\n",
    "    return post_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP - Preprocessing with spaCy\n",
    "\n",
    "Through preprocessing, I normalized the text:\n",
    "- lowercase\n",
    "- remove stopwords (words that doesn't add much meaning to the sentence) and punctuation symbols\n",
    "- [lemmatizes](https://en.wikipedia.org/wiki/Lemmatisation) each token.\n",
    "\n",
    "This way, the named entities extracted can easily be grouped to compute the number of time it is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://realpython.com/natural-language-processing-spacy-python/\n",
    "def is_token_allowed(token):\n",
    "    if not token or not token.string.strip() or token.is_stop or token.is_punct:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def preprocess_token(token):\n",
    "    # Reduce token to its lowercase lemma form\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "def normalize(id, post):\n",
    "    nlp_text = nlp(post)\n",
    "    get_post_entity(id, nlp_text)\n",
    "\n",
    "    complete_filtered_tokens = [\n",
    "        preprocess_token(token) for token in nlp_text if is_token_allowed(token)\n",
    "    ]\n",
    "\n",
    "    lemmatized_sentence = \" \".join(complete_filtered_tokens)\n",
    "\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the post invokes `get_post_entity`, triggering the extraction of named entities into csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PRAW to connect to Reddit\n",
    "\n",
    "[PRAW](https://praw.readthedocs.io/en/latest/) is the Python Reddit API Wrapper that we used to data-mine the _subreddits_.\n",
    "I'm not going into the details of the data scraping.  \n",
    "You can check out the original article on how you can get the [API key](https://medium.com/swlh/dashboards-in-python-using-dash-creating-a-data-table-using-data-from-reddit-1d6c0cecb4bd#08db).  \n",
    "I've excluded the config file from the source :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reddit connection\n",
    "reddit = praw.Reddit(\n",
    "    client_id=config.cid, client_secret=config.csec, user_agent=config.ua\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do more analysis, I created a function to get the latest postings. This function will allow me to configure the number of postings to download, and from which subreddit to download the postings from.\n",
    "\n",
    "Unlike the original article, I decided to format `created` into _datetime_ and splitted it into `created_date` and `created_time` as I figured this would allow me to have consolidated view on date level or drill down to the time level.\n",
    "\n",
    "Referencing the [PRAW documentation](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html#submission) for the submission attributes, I added _id_ and _author_ to the list as well. Important to note is that for real-time, I will be polling the latest 100 postings at a 30 seconds intervals.  \n",
    "Potentially there will be duplicated postings across the pollings. Storing the _id_ allows me to leverage on atoti's capability to update existing posts instead of inserting duplicated postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_posts(subreddit, count):\n",
    "    # list for df conversion\n",
    "    _posts = []\n",
    "    # return 100 new posts from wallstreetbets\n",
    "    new_bets = reddit.subreddit(subreddit).new(limit=count)\n",
    "    # return the important attributes\n",
    "    for post in new_bets:\n",
    "        # normalize the post and perform NLP named entities extraction\n",
    "        lemmatized_post = normalize(post.id, post.selftext)\n",
    "        _posts.append(\n",
    "            [\n",
    "                post.id,\n",
    "                post.author,\n",
    "                post.title,\n",
    "                post.score,\n",
    "                post.num_comments,\n",
    "                post.selftext,\n",
    "                lemmatized_post,\n",
    "                post.created,\n",
    "                post.pinned,\n",
    "                post.total_awards_received,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # create a dataframe\n",
    "    _posts = pd.DataFrame(\n",
    "        _posts,\n",
    "        columns=[\n",
    "            \"id\",\n",
    "            \"author\",\n",
    "            \"title\",\n",
    "            \"score\",\n",
    "            \"comments\",\n",
    "            \"post\",\n",
    "            \"lemmatized post\",\n",
    "            \"created\",\n",
    "            \"pinned\",\n",
    "            \"total awards\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    _posts[\"created\"] = pd.to_datetime(_posts[\"created\"], unit=\"s\")\n",
    "    _posts[\"created date\"] = pd.to_datetime(_posts[\"created\"], unit=\"s\").dt.date\n",
    "    _posts[\"created time\"] = pd.to_datetime(_posts[\"created\"], unit=\"s\").dt.time\n",
    "    _posts[\"subreddit\"] = subreddit\n",
    "\n",
    "    return _posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kickstart, I test downloaded the newest 500 post from the subreddit _wallstreetbets_.  \n",
    "This data will be loaded into atoti's in-memory datacube later on. This means that the amount of data is limited only by the hosting machine's capacity.  \n",
    "You could adjust the number of postings you want to look at according to your machine's memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>post</th>\n",
       "      <th>lemmatized post</th>\n",
       "      <th>created</th>\n",
       "      <th>pinned</th>\n",
       "      <th>total awards</th>\n",
       "      <th>created date</th>\n",
       "      <th>created time</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id70fe</td>\n",
       "      <td>algoman10</td>\n",
       "      <td>The Next big short CHINA</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Fellow autist,\\n\\nThe next big short is slowly...</td>\n",
       "      <td>fellow autist big short slowly brew scene cons...</td>\n",
       "      <td>2020-08-20 17:19:43</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>17:19:43</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id6u5b</td>\n",
       "      <td>caporalfourrier</td>\n",
       "      <td>New COD teaser out!</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[COD Cold War](https://youtu.be/m1kfCGjOaSw)\\n...</td>\n",
       "      <td>cod cold war](https://youtu.be m1kfcgjoasw tim...</td>\n",
       "      <td>2020-08-20 17:04:23</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>17:04:23</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id6eew</td>\n",
       "      <td>Not99Percent</td>\n",
       "      <td>The Journey of NVDA</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>July, 19 Quarter earnings announcement - 15th ...</td>\n",
       "      <td>july 19 quarter earning announcement 15th augu...</td>\n",
       "      <td>2020-08-20 16:23:32</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>16:23:32</td>\n",
       "      <td>wallstreetbets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           author                     title  score  comments  \\\n",
       "0  id70fe        algoman10  The Next big short CHINA      3         2   \n",
       "1  id6u5b  caporalfourrier       New COD teaser out!      2         8   \n",
       "2  id6eew     Not99Percent       The Journey of NVDA      1         5   \n",
       "\n",
       "                                                post  \\\n",
       "0  Fellow autist,\\n\\nThe next big short is slowly...   \n",
       "1  [COD Cold War](https://youtu.be/m1kfCGjOaSw)\\n...   \n",
       "2  July, 19 Quarter earnings announcement - 15th ...   \n",
       "\n",
       "                                     lemmatized post             created  \\\n",
       "0  fellow autist big short slowly brew scene cons... 2020-08-20 17:19:43   \n",
       "1  cod cold war](https://youtu.be m1kfcgjoasw tim... 2020-08-20 17:04:23   \n",
       "2  july 19 quarter earning announcement 15th augu... 2020-08-20 16:23:32   \n",
       "\n",
       "   pinned  total awards created date created time       subreddit  \n",
       "0   False             0   2020-08-20     17:19:43  wallstreetbets  \n",
       "1   False             0   2020-08-20     17:04:23  wallstreetbets  \n",
       "2   False             0   2020-08-20     16:23:32  wallstreetbets  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = get_latest_posts(\"wallstreetbets\", 500)\n",
    "# return top 3 df rows\n",
    "posts.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I adopted the below code from the original article. It uses Pandas dataframe to compute some simple statistics about the text in the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_features(_posts):\n",
    "    # copy the dataframe\n",
    "    df = _posts.copy()\n",
    "    # count words in post\n",
    "    df[\"words\"] = df[\"post\"].apply(lambda x: len(x.split()))\n",
    "    # count characters in post\n",
    "    df[\"chars\"] = df[\"post\"].apply(lambda x: len(x.replace(\" \", \"\")))\n",
    "    # calculate word density\n",
    "    df[\"word density\"] = (df[\"words\"] / (df[\"chars\"] + 1)).round(3)\n",
    "    # count unique words\n",
    "    df[\"unique words\"] = df[\"post\"].apply(lambda x: len(set(w for w in x.split())))\n",
    "    # percent of unique words\n",
    "    df[\"unique density\"] = (df[\"unique words\"] / df[\"words\"]).round(3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the function above on the posts that I have downloaded, we will get a dataset with similar structure to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>post</th>\n",
       "      <th>lemmatized post</th>\n",
       "      <th>created</th>\n",
       "      <th>pinned</th>\n",
       "      <th>total awards</th>\n",
       "      <th>created date</th>\n",
       "      <th>created time</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>word density</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id70fe</td>\n",
       "      <td>algoman10</td>\n",
       "      <td>The Next big short CHINA</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Fellow autist,\\n\\nThe next big short is slowly...</td>\n",
       "      <td>fellow autist big short slowly brew scene cons...</td>\n",
       "      <td>2020-08-20 17:19:43</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>17:19:43</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>336</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.183</td>\n",
       "      <td>209</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id6u5b</td>\n",
       "      <td>caporalfourrier</td>\n",
       "      <td>New COD teaser out!</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[COD Cold War](https://youtu.be/m1kfCGjOaSw)\\n...</td>\n",
       "      <td>cod cold war](https://youtu.be m1kfcgjoasw tim...</td>\n",
       "      <td>2020-08-20 17:04:23</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>17:04:23</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>30</td>\n",
       "      <td>163</td>\n",
       "      <td>0.183</td>\n",
       "      <td>27</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id6eew</td>\n",
       "      <td>Not99Percent</td>\n",
       "      <td>The Journey of NVDA</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>July, 19 Quarter earnings announcement - 15th ...</td>\n",
       "      <td>july 19 quarter earning announcement 15th augu...</td>\n",
       "      <td>2020-08-20 16:23:32</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>16:23:32</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>100</td>\n",
       "      <td>560</td>\n",
       "      <td>0.178</td>\n",
       "      <td>41</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           author                     title  score  comments  \\\n",
       "0  id70fe        algoman10  The Next big short CHINA      3         2   \n",
       "1  id6u5b  caporalfourrier       New COD teaser out!      2         8   \n",
       "2  id6eew     Not99Percent       The Journey of NVDA      1         5   \n",
       "\n",
       "                                                post  \\\n",
       "0  Fellow autist,\\n\\nThe next big short is slowly...   \n",
       "1  [COD Cold War](https://youtu.be/m1kfCGjOaSw)\\n...   \n",
       "2  July, 19 Quarter earnings announcement - 15th ...   \n",
       "\n",
       "                                     lemmatized post             created  \\\n",
       "0  fellow autist big short slowly brew scene cons... 2020-08-20 17:19:43   \n",
       "1  cod cold war](https://youtu.be m1kfcgjoasw tim... 2020-08-20 17:04:23   \n",
       "2  july 19 quarter earning announcement 15th augu... 2020-08-20 16:23:32   \n",
       "\n",
       "   pinned  total awards created date created time       subreddit  words  \\\n",
       "0   False             0   2020-08-20     17:19:43  wallstreetbets    336   \n",
       "1   False             0   2020-08-20     17:04:23  wallstreetbets     30   \n",
       "2   False             0   2020-08-20     16:23:32  wallstreetbets    100   \n",
       "\n",
       "   chars  word density  unique words  unique density  \n",
       "0   1839         0.183           209           0.622  \n",
       "1    163         0.183            27           0.900  \n",
       "2    560         0.178            41           0.410  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_nlp_features(posts)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## atoti cube creation for NLP analysis\n",
    "\n",
    "Finally we can get started with atoti. Let's create a session with the [sampling_mode](https://docs.atoti.io/0.4.2/lib/atoti.html#module-atoti.sampling) to set full, so that I can see all the data once it is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_config(metadata_db=\"./metadata.db\", sampling_mode=tt.sampling.FULL)\n",
    "session = tt.create_session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### atoti datastores\n",
    "\n",
    "I will create a `Reddit Posts` store for the data as per the original article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>post</th>\n",
       "      <th>lemmatized post</th>\n",
       "      <th>created</th>\n",
       "      <th>pinned</th>\n",
       "      <th>total awards</th>\n",
       "      <th>created date</th>\n",
       "      <th>created time</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>words</th>\n",
       "      <th>chars</th>\n",
       "      <th>word density</th>\n",
       "      <th>unique words</th>\n",
       "      <th>unique density</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id70fe</th>\n",
       "      <td>algoman10</td>\n",
       "      <td>The Next big short CHINA</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Fellow autist,\\r\\n\\r\\nThe next big short is sl...</td>\n",
       "      <td>fellow autist big short slowly brew scene cons...</td>\n",
       "      <td>2020-08-20T17:19:43</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>17:19:43</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>336</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.183</td>\n",
       "      <td>209</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id6u5b</th>\n",
       "      <td>caporalfourrier</td>\n",
       "      <td>New COD teaser out!</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[COD Cold War](https://youtu.be/m1kfCGjOaSw)\\r...</td>\n",
       "      <td>cod cold war](https://youtu.be m1kfcgjoasw tim...</td>\n",
       "      <td>2020-08-20T17:04:23</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>17:04:23</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>30</td>\n",
       "      <td>163</td>\n",
       "      <td>0.183</td>\n",
       "      <td>27</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id6eew</th>\n",
       "      <td>Not99Percent</td>\n",
       "      <td>The Journey of NVDA</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>July, 19 Quarter earnings announcement - 15th ...</td>\n",
       "      <td>july 19 quarter earning announcement 15th augu...</td>\n",
       "      <td>2020-08-20T16:23:32</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>16:23:32</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>100</td>\n",
       "      <td>560</td>\n",
       "      <td>0.178</td>\n",
       "      <td>41</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id6cfx</th>\n",
       "      <td>jacektrocinski</td>\n",
       "      <td>Why Stock Will Continue To Moon</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>* TINA (There Is No Alternative) to equities.\\...</td>\n",
       "      <td>tina alternative equity rebound march low cent...</td>\n",
       "      <td>2020-08-20T16:18:37</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>16:18:37</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>77</td>\n",
       "      <td>367</td>\n",
       "      <td>0.209</td>\n",
       "      <td>64</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id62kp</th>\n",
       "      <td>LazyMeal</td>\n",
       "      <td>Bulls üêÇ and üåàüêª GATHER HERE!</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Are you a bull and love to make money on stonk...</td>\n",
       "      <td>bull love money stonk üåà üêª like suck dude money...</td>\n",
       "      <td>2020-08-20T15:53:38</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>15:53:38</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>119</td>\n",
       "      <td>508</td>\n",
       "      <td>0.234</td>\n",
       "      <td>89</td>\n",
       "      <td>0.748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                            title  score  comments  \\\n",
       "id                                                                          \n",
       "id70fe        algoman10         The Next big short CHINA      3         2   \n",
       "id6u5b  caporalfourrier              New COD teaser out!      2         8   \n",
       "id6eew     Not99Percent              The Journey of NVDA      1         5   \n",
       "id6cfx   jacektrocinski  Why Stock Will Continue To Moon      5         7   \n",
       "id62kp         LazyMeal      Bulls üêÇ and üåàüêª GATHER HERE!      3         5   \n",
       "\n",
       "                                                     post  \\\n",
       "id                                                          \n",
       "id70fe  Fellow autist,\\r\\n\\r\\nThe next big short is sl...   \n",
       "id6u5b  [COD Cold War](https://youtu.be/m1kfCGjOaSw)\\r...   \n",
       "id6eew  July, 19 Quarter earnings announcement - 15th ...   \n",
       "id6cfx  * TINA (There Is No Alternative) to equities.\\...   \n",
       "id62kp  Are you a bull and love to make money on stonk...   \n",
       "\n",
       "                                          lemmatized post  \\\n",
       "id                                                          \n",
       "id70fe  fellow autist big short slowly brew scene cons...   \n",
       "id6u5b  cod cold war](https://youtu.be m1kfcgjoasw tim...   \n",
       "id6eew  july 19 quarter earning announcement 15th augu...   \n",
       "id6cfx  tina alternative equity rebound march low cent...   \n",
       "id62kp  bull love money stonk üåà üêª like suck dude money...   \n",
       "\n",
       "                    created  pinned  total awards created date created time  \\\n",
       "id                                                                            \n",
       "id70fe  2020-08-20T17:19:43   False             0   2020-08-20     17:19:43   \n",
       "id6u5b  2020-08-20T17:04:23   False             0   2020-08-20     17:04:23   \n",
       "id6eew  2020-08-20T16:23:32   False             0   2020-08-20     16:23:32   \n",
       "id6cfx  2020-08-20T16:18:37   False             0   2020-08-20     16:18:37   \n",
       "id62kp  2020-08-20T15:53:38   False             0   2020-08-20     15:53:38   \n",
       "\n",
       "             subreddit  words  chars  word density  unique words  \\\n",
       "id                                                                 \n",
       "id70fe  wallstreetbets    336   1839         0.183           209   \n",
       "id6u5b  wallstreetbets     30    163         0.183            27   \n",
       "id6eew  wallstreetbets    100    560         0.178            41   \n",
       "id6cfx  wallstreetbets     77    367         0.209            64   \n",
       "id62kp  wallstreetbets    119    508         0.234            89   \n",
       "\n",
       "        unique density  \n",
       "id                      \n",
       "id70fe           0.622  \n",
       "id6u5b           0.900  \n",
       "id6eew           0.410  \n",
       "id6cfx           0.831  \n",
       "id62kp           0.748  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_store = session.read_pandas(df, keys=[\"id\"], store_name=\"Reddit Posts\")\n",
    "reddit_store.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my experiment with spaCy, I create a `Entities` store to store the named entities.  \n",
    "Below, I set `watch=True` for the entities.csv such that any update to the file will trigger an upload to the datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_store = session.read_csv(\n",
    "    entity_file_name,\n",
    "    keys=[\"id\", \"category\", \"text\"],\n",
    "    store_name=\"Entities\",\n",
    "    watch=True,\n",
    "    sep=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">id70fe</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Organization</th>\n",
       "      <th>Trump</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSBC</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Geographical Location</th>\n",
       "      <th>U.S</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hong Kong</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text count\n",
       "id     category              text                 \n",
       "id70fe Organization          Trump               1\n",
       "                             HSBC                1\n",
       "       Geographical Location U.S                 1\n",
       "                             China               5\n",
       "                             Hong Kong           3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_store.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I join the `Entities` store to `Reddit Posts` which I decided to use as the base store.  \n",
    "Note that atoti automatically joins columns with the same name across both stores. You could define your own [mapping](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=join#atoti.store.Store.join) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_store.join(entity_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### atoti Cube creation - No measures\n",
    "\n",
    "By default, atoti will automatically creates hierarchies and measures from the datastore based on the data type of the columns.  \n",
    "In this case, I prefer to manually create my measures, hence I set the [mode](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=create_cube#atoti.session.Session.create_cube) of creation to `no_measures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = session.create_cube(reddit_store, \"Reddit\", mode=\"no_measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"426px\" height=\"719px\" viewBox=\"0.00 0.00 426.00 719.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1.0 1.0) rotate(0) translate(4 715)\">\n",
       "<title>Reddit schema</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-715 422,-715 422,4 -4,4\"/>\n",
       "<!-- Reddit Posts -->\n",
       "<g id=\"node1\" class=\"node\"><title>Reddit Posts</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-669.5 8,-706.5 168,-706.5 168,-669.5 8,-669.5\"/>\n",
       "<text text-anchor=\"start\" x=\"49\" y=\"-685.3\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Reddit Posts</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-632.5 8,-669.5 168,-669.5 168,-632.5 8,-632.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-648.3\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"44\" y=\"-648.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> ¬†id </text>\n",
       "<text text-anchor=\"start\" x=\"66\" y=\"-648.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"102\" y=\"-648.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-595.5 8,-632.5 168,-632.5 168,-595.5 8,-595.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-611.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">author </text>\n",
       "<text text-anchor=\"start\" x=\"59\" y=\"-611.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"95\" y=\"-611.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-558.5 8,-595.5 168,-595.5 168,-558.5 8,-558.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-574.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">title </text>\n",
       "<text text-anchor=\"start\" x=\"43\" y=\"-574.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"79\" y=\"-574.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-521.5 8,-558.5 168,-558.5 168,-521.5 8,-521.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-537.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">score </text>\n",
       "<text text-anchor=\"start\" x=\"54\" y=\"-537.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-537.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-484.5 8,-521.5 168,-521.5 168,-484.5 8,-484.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-500.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">comments </text>\n",
       "<text text-anchor=\"start\" x=\"81\" y=\"-500.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"107\" y=\"-500.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-447.5 8,-484.5 168,-484.5 168,-447.5 8,-447.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-463.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">post </text>\n",
       "<text text-anchor=\"start\" x=\"48\" y=\"-463.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"84\" y=\"-463.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-410.5 8,-447.5 168,-447.5 168,-410.5 8,-410.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-426.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">lemmatized post </text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-426.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"150\" y=\"-426.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-373.5 8,-410.5 168,-410.5 168,-373.5 8,-373.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">created </text>\n",
       "<text text-anchor=\"start\" x=\"64\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">LocalDateTime</text>\n",
       "<text text-anchor=\"start\" x=\"153\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-336.5 8,-373.5 168,-373.5 168,-336.5 8,-336.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pinned </text>\n",
       "<text text-anchor=\"start\" x=\"61\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">boolean</text>\n",
       "<text text-anchor=\"start\" x=\"107\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-299.5 8,-336.5 168,-336.5 168,-299.5 8,-299.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">total awards </text>\n",
       "<text text-anchor=\"start\" x=\"91\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"117\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-262.5 8,-299.5 168,-299.5 168,-262.5 8,-262.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-278.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">created date </text>\n",
       "<text text-anchor=\"start\" x=\"91\" y=\"-278.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">LocalDate</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-278.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-225.5 8,-262.5 168,-262.5 168,-225.5 8,-225.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-241.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">created time </text>\n",
       "<text text-anchor=\"start\" x=\"91\" y=\"-241.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"127\" y=\"-241.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-188.5 8,-225.5 168,-225.5 168,-188.5 8,-188.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-204.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">subreddit </text>\n",
       "<text text-anchor=\"start\" x=\"76\" y=\"-204.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"112\" y=\"-204.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-151.5 8,-188.5 168,-188.5 168,-151.5 8,-151.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-167.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">words </text>\n",
       "<text text-anchor=\"start\" x=\"59\" y=\"-167.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"85\" y=\"-167.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-114.5 8,-151.5 168,-151.5 168,-114.5 8,-114.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-130.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">chars </text>\n",
       "<text text-anchor=\"start\" x=\"53\" y=\"-130.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"79\" y=\"-130.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-77.5 8,-114.5 168,-114.5 168,-77.5 8,-77.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-93.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">word density </text>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-93.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">double</text>\n",
       "<text text-anchor=\"start\" x=\"136\" y=\"-93.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-40.5 8,-77.5 168,-77.5 168,-40.5 8,-40.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-56.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">unique words </text>\n",
       "<text text-anchor=\"start\" x=\"99\" y=\"-56.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">long</text>\n",
       "<text text-anchor=\"start\" x=\"125\" y=\"-56.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-3.5 8,-40.5 168,-40.5 168,-3.5 8,-3.5\"/>\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-19.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">unique density </text>\n",
       "<text text-anchor=\"start\" x=\"104\" y=\"-19.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">double</text>\n",
       "<text text-anchor=\"start\" x=\"144\" y=\"-19.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- Entities -->\n",
       "<g id=\"node2\" class=\"node\"><title>Entities</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-410.5 263.5,-447.5 410.5,-447.5 410.5,-410.5 263.5,-410.5\"/>\n",
       "<text text-anchor=\"start\" x=\"313.5\" y=\"-426.3\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Entities</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-373.5 263.5,-410.5 410.5,-410.5 410.5,-373.5 263.5,-373.5\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> ¬†id </text>\n",
       "<text text-anchor=\"start\" x=\"321.5\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"357.5\" y=\"-389.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-336.5 263.5,-373.5 410.5,-373.5 410.5,-336.5 263.5,-336.5\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> ¬†category </text>\n",
       "<text text-anchor=\"start\" x=\"359.5\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"395.5\" y=\"-352.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-299.5 263.5,-336.5 410.5,-336.5 410.5,-299.5 263.5,-299.5\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-weight=\"bold\" font-size=\"14.00\">Key</text>\n",
       "<text text-anchor=\"start\" x=\"299.5\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> ¬†text </text>\n",
       "<text text-anchor=\"start\" x=\"331.5\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">String</text>\n",
       "<text text-anchor=\"start\" x=\"367.5\" y=\"-315.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"263.5,-262.5 263.5,-299.5 410.5,-299.5 410.5,-262.5 263.5,-262.5\"/>\n",
       "<text text-anchor=\"start\" x=\"274.5\" y=\"-278.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">text count </text>\n",
       "<text text-anchor=\"start\" x=\"334.5\" y=\"-278.3\" font-family=\"Times New Roman,serif\" font-style=\"italic\" font-size=\"14.00\">int</text>\n",
       "<text text-anchor=\"start\" x=\"350.5\" y=\"-278.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- Reddit Posts&#45;&gt;Entities -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>Reddit Posts-&gt;Entities</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M176.254,-355.5C198.473,-355.5 222.402,-355.5 244.79,-355.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.955,-359 254.955,-355.5 244.955,-352 244.955,-359\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-359.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">id ‚Üí id</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the structure of the cube created. Only a `Count` measure is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Dimensions": {
        "Hierarchies": {
         "author": [
          "author"
         ],
         "category": [
          "category"
         ],
         "chars": [
          "chars"
         ],
         "comments": [
          "comments"
         ],
         "created": [
          "created"
         ],
         "created date": [
          "created date"
         ],
         "created time": [
          "created time"
         ],
         "id": [
          "id"
         ],
         "lemmatized post": [
          "lemmatized post"
         ],
         "pinned": [
          "pinned"
         ],
         "post": [
          "post"
         ],
         "score": [
          "score"
         ],
         "subreddit": [
          "subreddit"
         ],
         "text": [
          "text"
         ],
         "text count": [
          "text count"
         ],
         "title": [
          "title"
         ],
         "total awards": [
          "total awards"
         ],
         "unique words": [
          "unique words"
         ],
         "words": [
          "words"
         ]
        }
       },
       "Measures": {
        "contributors.COUNT": {
         "formatter": null,
         "visible": true
        }
       }
      },
      "text/html": [
       "<ul>\n",
       "<li>Reddit\n",
       "  <ul>\n",
       "  <li>Dimensions\n",
       "    <ul>\n",
       "    <li>Hierarchies\n",
       "      <ul>\n",
       "      <li>author\n",
       "        <ol>\n",
       "        <li>author</li>\n",
       "        </ol>      </li>\n",
       "      <li>category\n",
       "        <ol>\n",
       "        <li>category</li>\n",
       "        </ol>      </li>\n",
       "      <li>chars\n",
       "        <ol>\n",
       "        <li>chars</li>\n",
       "        </ol>      </li>\n",
       "      <li>comments\n",
       "        <ol>\n",
       "        <li>comments</li>\n",
       "        </ol>      </li>\n",
       "      <li>created\n",
       "        <ol>\n",
       "        <li>created</li>\n",
       "        </ol>      </li>\n",
       "      <li>created date\n",
       "        <ol>\n",
       "        <li>created date</li>\n",
       "        </ol>      </li>\n",
       "      <li>created time\n",
       "        <ol>\n",
       "        <li>created time</li>\n",
       "        </ol>      </li>\n",
       "      <li>id\n",
       "        <ol>\n",
       "        <li>id</li>\n",
       "        </ol>      </li>\n",
       "      <li>lemmatized post\n",
       "        <ol>\n",
       "        <li>lemmatized post</li>\n",
       "        </ol>      </li>\n",
       "      <li>pinned\n",
       "        <ol>\n",
       "        <li>pinned</li>\n",
       "        </ol>      </li>\n",
       "      <li>post\n",
       "        <ol>\n",
       "        <li>post</li>\n",
       "        </ol>      </li>\n",
       "      <li>score\n",
       "        <ol>\n",
       "        <li>score</li>\n",
       "        </ol>      </li>\n",
       "      <li>subreddit\n",
       "        <ol>\n",
       "        <li>subreddit</li>\n",
       "        </ol>      </li>\n",
       "      <li>text\n",
       "        <ol>\n",
       "        <li>text</li>\n",
       "        </ol>      </li>\n",
       "      <li>text count\n",
       "        <ol>\n",
       "        <li>text count</li>\n",
       "        </ol>      </li>\n",
       "      <li>title\n",
       "        <ol>\n",
       "        <li>title</li>\n",
       "        </ol>      </li>\n",
       "      <li>total awards\n",
       "        <ol>\n",
       "        <li>total awards</li>\n",
       "        </ol>      </li>\n",
       "      <li>unique words\n",
       "        <ol>\n",
       "        <li>unique words</li>\n",
       "        </ol>      </li>\n",
       "      <li>words\n",
       "        <ol>\n",
       "        <li>words</li>\n",
       "        </ol>      </li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  <li>Measures\n",
       "    <ul>\n",
       "    <li>contributors.COUNT\n",
       "      <ul>\n",
       "      <li>formatter: None</li>\n",
       "      <li>visible: True</li>\n",
       "      </ul>\n",
       "    </li>\n",
       "    </ul>\n",
       "  </li>\n",
       "  </ul>\n",
       "</li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<atoti.cube.Cube at 0x20544a31bb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "Reddit"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = cube.measures\n",
    "l = cube.levels\n",
    "h = cube.hierarchies\n",
    "cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing hierarchies\n",
    "\n",
    "Since I am going to create my own measures, I will delete the numeric columns from my hierarchies to avoid confusion with the measures that I will be creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del h[\"text count\"]\n",
    "del h[\"score\"]\n",
    "del h[\"comments\"]\n",
    "del h[\"total awards\"]\n",
    "del h[\"words\"]\n",
    "del h[\"chars\"]\n",
    "del h[\"unique words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measures creation\n",
    "\n",
    "In default `auto` mode, atoti will create a `MEAN` and a `SUM` measure.  \n",
    "However, as I only require `sum` in my simple use case and in order to demonstrate how simple aggregation can be done with atoti, I created the below measures instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"Text Count\"] = tt.agg.sum(entity_store[\"text count\"])\n",
    "m[\"Score\"] = tt.agg.sum(reddit_store[\"score\"])\n",
    "m[\"Comments\"] = tt.agg.sum(reddit_store[\"comments\"])\n",
    "m[\"Total awards\"] = tt.agg.sum(reddit_store[\"total awards\"])\n",
    "m[\"Words\"] = tt.agg.sum(reddit_store[\"words\"])\n",
    "m[\"Chars\"] = tt.agg.sum(reddit_store[\"chars\"])\n",
    "m[\"Unique words\"] = tt.agg.sum(reddit_store[\"unique words\"])\n",
    "m[\"Word density\"] = tt.agg.sum(reddit_store[\"word density\"])\n",
    "m[\"Unique density\"] = tt.agg.sum(reddit_store[\"unique density\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distinct measure\n",
    "\n",
    "I use the [`count_distinct`](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=count_distinct#atoti.agg.count_distinct) function to compute the number of days the data span across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"Number days\"] = tt.agg.count_distinct(reddit_store[\"created date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cumulative measure\n",
    "\n",
    "Aggregation with the [`cumulatve`](https://docs.atoti.io/0.4.2/lib/atoti.scope.html?highlight=cumulative#atoti.scope.cumulative) scope allows me to see the trend of the posting over the `created` level. By setting the parameter `dense=True`, we can see a continous plot of the trend even if there are no postings on days in between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"Cumulative Count\"] = tt.agg.sum(\n",
    "    m[\"contributors.COUNT\"], scope=tt.scope.cumulative(l[\"created\"], dense=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "atoti": {
     "state": {
      "name": "Posting Trends",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "mapping": {
          "horizontalSubplots": [],
          "splitBy": [
           "[Hierarchies].[subreddit].[subreddit]"
          ],
          "values": [
           "[Measures].[Cumulative Count]"
          ],
          "verticalSubplots": [],
          "xAxis": [
           "[Hierarchies].[created].[created]"
          ]
         },
         "plotly": {
          "data": {
           "commonTraceOverride": {
            "mode": "lines"
           }
          }
         },
         "type": "plotly-line-chart"
        },
        "query": {
         "mdx": "SELECT NON EMPTY Crossjoin([Hierarchies].[created].[created].Members, [Hierarchies].[subreddit].[subreddit].Members) ON ROWS, NON EMPTY [Measures].[Cumulative Count] ON COLUMNS FROM [Reddit] CELL PROPERTIES VALUE, FORMATTED_VALUE, BACK_COLOR, FORE_COLOR, FONT_FLAGS",
         "serverUrl": "",
         "updateMode": "once"
        }
       },
       "containerKey": "chart",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.atoti.v0+json": {
       "cube": "Reddit",
       "name": "Posting Trends",
       "session": "Unnamed"
      },
      "text/plain": [
       "Install and enable the atoti JupyterLab extension to see this widget."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube.visualize(\"Posting Trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## atoti Pivot table against Dash DataTable\n",
    "\n",
    "Now the moment of truth! How will atoti data table fare compared to the [Dash DataTable](https://medium.com/swlh/dashboards-in-python-using-dash-creating-a-data-table-using-data-from-reddit-1d6c0cecb4bd#be7c) in the original article? _Maybe_ Dash Datatable has evolved since the article was last published, let's just enjoy the atoti table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "atoti": {
     "height": 613,
     "state": {
      "name": "Reddit Data Table",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "tabular": {
          "cellRenderers": [],
          "columns": [
           {
            "key": "[Hierarchies].[subreddit].[subreddit]",
            "width": 81
           },
           {
            "key": "[Hierarchies].[title].[title]",
            "width": 308
           },
           {
            "key": "[Hierarchies].[pinned].[pinned]",
            "width": 80
           },
           {
            "key": "[Hierarchies].[created].[created]",
            "width": 132
           },
           {
            "key": "[Measures].[Chars]",
            "width": 80
           },
           {
            "key": "[Measures].[Comments (1)]",
            "width": 80
           },
           {
            "key": "[Measures].[Score (1)]",
            "width": 80
           },
           {
            "key": "[Measures].[Total awards]",
            "width": 80
           },
           {
            "key": "[Measures].[Unique density]",
            "width": 80
           },
           {
            "key": "[Measures].[Unique words]",
            "width": 80
           },
           {
            "key": "[Measures].[Word density]",
            "width": 80
           }
          ],
          "columnsGroups": [
           {
            "captionProducer": "firstColumn",
            "cellFactory": "kpi-status",
            "selector": "kpi-status"
           },
           {
            "captionProducer": "firstColumn",
            "cellFactory": "lookup",
            "selector": "lookup"
           },
           {
            "captionProducer": "expiry",
            "cellFactory": "expiry",
            "selector": "kpi-expiry"
           }
          ],
          "defaultOptions": {},
          "expansion": {
           "automaticExpansion": true
          },
          "hideAddButton": true,
          "pinnedHeaderSelector": "member",
          "sortingMode": "breaking",
          "statisticsShown": true
         }
        },
        "contextValues": {},
        "mdx": "WITH  Member [Measures].[Comments (1)] AS [Measures].[Comments], BACK_COLOR = CASE WHEN [Measures].[Comments (1)] >= 50 THEN rgb(217, 234, 211) WHEN [Measures].[Comments (1)] <= 20 THEN rgb(244, 204, 204) END, FORE_COLOR = CASE WHEN [Measures].[Comments (1)] >= 50 THEN rgb(39, 78, 19) WHEN [Measures].[Comments (1)] <= 20 THEN rgb(102, 0, 0) END, FONT_FLAGS = CASE WHEN [Measures].[Comments (1)] >= 50 THEN 1 WHEN [Measures].[Comments (1)] <= 20 THEN 1 END, FORMAT_STRING = \"#,###\", CAPTION = [Measures].[Comments].MEMBER_CAPTION    Member [Measures].[Score (1)] AS [Measures].[Score], BACK_COLOR = CASE WHEN [Measures].[Score (1)] >= 50 THEN rgb(217, 234, 211) WHEN [Measures].[Score (1)] <= 20 THEN rgb(252, 229, 205) END, FORE_COLOR = CASE WHEN [Measures].[Score (1)] >= 50 THEN rgb(39, 78, 19) WHEN [Measures].[Score (1)] <= 20 THEN rgb(180, 95, 6) END, FONT_FLAGS = CASE WHEN [Measures].[Score (1)] >= 50 THEN 1 WHEN [Measures].[Score (1)] <= 20 THEN 1 END, FORMAT_STRING = \"#,###\", CAPTION = [Measures].[Score].MEMBER_CAPTION  SELECT NON EMPTY {[Measures].[Chars], [Measures].[Comments (1)], [Measures].[Score (1)], [Measures].[Total awards], [Measures].[Unique density], [Measures].[Unique words], [Measures].[Word density]} ON COLUMNS, NON EMPTY Order(Crossjoin([Hierarchies].[subreddit].[subreddit].Members, [Hierarchies].[title].[title].Members, [Hierarchies].[post].[post].Members, [Hierarchies].[pinned].[pinned].Members, [Hierarchies].[created].[created].Members), [Measures].[Chars], BDESC) ON ROWS FROM [Reddit] CELL PROPERTIES BACK_COLOR, FONT_FLAGS, FORE_COLOR, FORMATTED_VALUE, VALUE",
        "ranges": {
         "column": {
          "chunkSize": 50,
          "thresholdPercentage": 0.2
         },
         "row": {
          "chunkSize": 2000,
          "thresholdPercentage": 0.1
         }
        },
        "serverUrl": "",
        "updateMode": "once"
       },
       "containerKey": "pivot-table",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.atoti.v0+json": {
       "cube": "Reddit",
       "name": "Reddit Data Table",
       "session": "Unnamed"
      },
      "text/plain": [
       "Install and enable the atoti JupyterLab extension to see this widget."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube.visualize(\"Reddit Data Table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can easily adjust the height of the table or the width of the columns. With the atoti Editor, I can easily:\n",
    "- add or remove the levels and measures\n",
    "- filter the data\n",
    "- sort the measures\n",
    "- format the columns\n",
    "\n",
    "Refer to the GIF below to see how these could be done.\n",
    "\n",
    "<img src=\"https://data.atoti.io/notebooks/reddit/DataTable.gif\" alt=\"atoti table\" width=\"1080\"/>\n",
    "\n",
    "I can toggle the table from a tabular table to pivot table. Pivot table allows me to drill down on any hierarchies. The measures are computed on the fly as the level changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "atoti": {
     "state": {
      "name": "",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "tabular": {
          "addButtonFilter": "numeric",
          "cellRenderers": [
           "tree-layout"
          ],
          "columnsGroups": [
           {
            "captionProducer": "firstColumn",
            "cellFactory": "kpi-status",
            "selector": "kpi-status"
           },
           {
            "captionProducer": "firstColumn",
            "cellFactory": "lookup",
            "selector": "lookup"
           },
           {
            "captionProducer": "expiry",
            "cellFactory": "expiry",
            "selector": "kpi-expiry"
           },
           {
            "captionProducer": "columnMerge",
            "cellFactory": {
             "args": {},
             "key": "treeCells"
            },
            "selector": "member"
           }
          ],
          "defaultOptions": {},
          "expansion": {
           "automaticExpansion": true
          },
          "hideAddButton": true,
          "pinnedHeaderSelector": "member",
          "sortingMode": "non-breaking",
          "statisticsShown": true
         }
        },
        "contextValues": {},
        "mdx": "SELECT NON EMPTY {[Measures].[Total awards], [Measures].[Comments], [Measures].[contributors.COUNT]} ON COLUMNS, NON EMPTY Crossjoin(Hierarchize(DrilldownLevel([Hierarchies].[subreddit].[ALL].[AllMember])), Hierarchize(DrilldownLevel([Hierarchies].[category].[ALL].[AllMember])), Hierarchize(DrilldownLevel([Hierarchies].[created date].[ALL].[AllMember]))) ON ROWS FROM [Reddit]",
        "ranges": {
         "column": {
          "chunkSize": 50,
          "thresholdPercentage": 0.2
         },
         "row": {
          "chunkSize": 2000,
          "thresholdPercentage": 0.1
         }
        },
        "serverUrl": "",
        "updateMode": "once"
       },
       "containerKey": "pivot-table",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.atoti.v0+json": {
       "cube": "Reddit",
       "name": null,
       "session": "Unnamed"
      },
      "text/plain": [
       "Install and enable the atoti JupyterLab extension to see this widget."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cube.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time dashboarding with atoti\n",
    "\n",
    "At the moment, I have only loaded data from the subreddit _wallstreetbets_. In the next few cell, I have created functions that allow me to poll the latest 100 postings from 3 subreddit groups:\n",
    "- android\n",
    "- wallstreetbets\n",
    "- datascience\n",
    "\n",
    "Let's switch over to a dashboard that I have prepared in advance by accessing the url from the next cell. After loading the dashboard, trigger the rest of the cells to start the data polling. You can then go back to the dashboard to see it getting refreshed each time new data is downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:53579/#/dashboard/c7c'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.url + \"/#/dashboard/c7c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used [`store.load_pandas`](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=load_pandas#atoti.store.Store.load_pandas) to load the Pandas dataframe incrementally into the `Reddit Posts` store. For the `Entities` store, the csv file is on `watch` mode, hence any update will trigger data to be uploaded into the datastore. Based on the stores' keys, existing data will be updated and new data will be inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_subreddit(subreddit):\n",
    "    posts_news = get_latest_posts(subreddit, 100)\n",
    "    df_news = get_nlp_features(posts_news)\n",
    "\n",
    "    reddit_store.load_pandas(df_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import threading\n",
    "\n",
    "should_publish = threading.Event()\n",
    "update_queue = queue.Queue()\n",
    "\n",
    "\n",
    "def start_publisher():\n",
    "    # Do 10 iterations with 30s interval\n",
    "    starttime = time.time()\n",
    "    print(\"Start polling\", starttime)\n",
    "    poll_iteration = 1\n",
    "\n",
    "    while poll_iteration <= 10:\n",
    "        print(\"\\rpublishing update \", end=\"\")\n",
    "        update_queue.put((poll_iteration))\n",
    "        poll_iteration += 1\n",
    "        time.sleep(30)\n",
    "        print(\"\\rawaiting for publishing update\", end=\"\")\n",
    "        should_publish.wait()\n",
    "        update_queue.join()\n",
    "\n",
    "\n",
    "def start_update_listener():\n",
    "    while True:\n",
    "        poll_iteration = update_queue.get()\n",
    "        print(poll_iteration, \">>> Start polling from subreddit... \")\n",
    "        # poll from subreddit\n",
    "        load_new_subreddit(\"android\")\n",
    "        load_new_subreddit(\"wallstreetbets\")\n",
    "        load_new_subreddit(\"datascience\")\n",
    "        print(poll_iteration, \">>> End polling from subreddit... \")\n",
    "        update_queue.task_done()\n",
    "\n",
    "\n",
    "listener_thread = threading.Thread(target=start_update_listener, daemon=True)\n",
    "publisher_thread = threading.Thread(target=start_publisher, daemon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start polling 1597916179.5443585\n",
      "publishing update 1 >>> Start polling from subreddit... \n"
     ]
    }
   ],
   "source": [
    "publisher_thread.start()\n",
    "listener_thread.start()\n",
    "# start publishing\n",
    "should_publish.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to stop the data polling before the 10 iterations end, run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 >>> End polling from subreddit... \n",
      "awaiting for publishing update"
     ]
    }
   ],
   "source": [
    "# pause publishing\n",
    "should_publish.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun exploring the 3 pages within the dashboard created! Do play around with some of the ootb widgets such as \"Quick Page Filter\" that allow you to easily filter the page from the dashboard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
