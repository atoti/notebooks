{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5138d3c8-8fbb-4556-9730-e490f0dcabe3",
   "metadata": {},
   "source": [
    "# Sourcing GHI data from NREL\n",
    "This notebook can be used to download data from the \n",
    "The National Renewable Energy Laboratory (NREL) \n",
    "Solar Radiation Database (NSRDB).\n",
    "\n",
    "For this to work, first install h5pyd:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45df2ddc-8694-40aa-a70e-684ac94abe25",
   "metadata": {},
   "source": [
    "$ pip3 install --user h5pyd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2442b-eabc-4a57-99c9-ef809f55a8ab",
   "metadata": {},
   "source": [
    "Next, configure HSDS: (in shell)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfa7fbdf-1625-4e13-aeb9-a651e2fe9370",
   "metadata": {},
   "source": [
    "$ hsconfigure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2a8711-8600-45e7-89f5-b66e135396ad",
   "metadata": {},
   "source": [
    "and enter at the prompt:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1c8414d-292f-446a-b608-87561e7c4c7c",
   "metadata": {},
   "source": [
    "hs_endpoint = https://developer.nrel.gov/api/hsds\n",
    "hs_username = None\n",
    "hs_password = None\n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef38f0-ffea-4396-9b3f-c5c1e51f8840",
   "metadata": {},
   "source": [
    "This notebook draws heavily from NREL github examples (https://github.com/NREL/hsds-examples/blob/master/notebooks/) and uses the example API key given there. Used here is the demonstation key and is rate-limited per IP.\n",
    "\n",
    "To get an API key, visit https://developer.nrel.gov/signup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf22b31-fcd7-4552-a9de-3e3cb84254ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import h5pyd  # to read from nrel server\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04393d06-ae6c-4f9b-8ebb-88fe79e9f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing(lst):\n",
    "    return [x for x in range(lst[0], lst[-1] + 1) if x not in lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e3bf6-523a-4342-bd43-78a269a39c21",
   "metadata": {},
   "source": [
    "While the wildfires are primarily located in California, winds and the jet stream means the smoke from a California fire can also impact neighboring states.  This and the next file are structured so GHI data for other states can just as easily be downloaded and studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabd95b0-c114-43e2-8523-b95ef533b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set state, year, time of day to download\n",
    "state = \"California\"\n",
    "start = 2016\n",
    "end = 2021\n",
    "utctime = \"20\"  # selecting noon local standard time\n",
    "k = 3  # keeping only 1/k^2 of the locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76698886-a3e1-4aa4-819c-5b0a6051f4c8",
   "metadata": {},
   "source": [
    "Connect to the NREL server to download the meta-data for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17907801-9bb0-4069-ba76-44156ec8d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_path = f\"/nrel/nsrdb/v3/nsrdb_{start}.h5\"\n",
    "with h5pyd.File(remote_path, mode=\"r\") as f:\n",
    "    meta = pd.DataFrame(f[\"meta\"][...])\n",
    "    meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7840146-cceb-4846-a882-d49b6883b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NSRDB pixels in California = 26010\n"
     ]
    }
   ],
   "source": [
    "state_meta = meta.loc[meta[\"state\"] == bytes(state, encoding=\"utf-8\")]\n",
    "state_index = state_meta.index.values.copy()\n",
    "print(f\"Number of NSRDB pixels in {state} = {len(state_meta)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4519b-0ec3-4004-a24b-fcb5c1a2fc26",
   "metadata": {},
   "source": [
    "Create a dateframe of the sensor station locations in latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3098284e-e150-4027-be00-ee8fd754db8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>station_000070276</td>\n",
       "      <td>32.529999</td>\n",
       "      <td>-117.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>station_000070588</td>\n",
       "      <td>32.570000</td>\n",
       "      <td>-117.099998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>station_000070589</td>\n",
       "      <td>32.570000</td>\n",
       "      <td>-117.059998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>station_000070590</td>\n",
       "      <td>32.570000</td>\n",
       "      <td>-117.019997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>station_000070591</td>\n",
       "      <td>32.570000</td>\n",
       "      <td>-116.980003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Station   Latitude   Longitude\n",
       "0  station_000070276  32.529999 -117.099998\n",
       "1  station_000070588  32.570000 -117.099998\n",
       "2  station_000070589  32.570000 -117.059998\n",
       "3  station_000070590  32.570000 -117.019997\n",
       "4  station_000070591  32.570000 -116.980003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latlon_data = []\n",
    "for index in state_index:\n",
    "    location_id = f\"station_{index:0=9d}\"\n",
    "    location_data = [location_id, meta[\"latitude\"][index], meta[\"longitude\"][index]]\n",
    "    latlon_data.append(location_data)\n",
    "\n",
    "latlon_df = pd.DataFrame(latlon_data, columns=[\"Station\", \"Latitude\", \"Longitude\"])\n",
    "latlon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2cb291-f798-4d88-b913-64944a5257a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = sorted(latlon_df.Latitude.unique())\n",
    "lon = sorted(latlon_df.Longitude.unique())\n",
    "\n",
    "# keeping only 1/k^2 of the station data\n",
    "keep_lats = lat[::k]\n",
    "keep_lons = lon[::k]\n",
    "\n",
    "final_latlon = latlon_df[\n",
    "    latlon_df[\"Latitude\"].isin(keep_lats) & latlon_df[\"Longitude\"].isin(keep_lons)\n",
    "]\n",
    "\n",
    "feather.write_feather(final_latlon, f\"./data/nsrdb_station_lat_lon.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74508c11-d979-44ff-89b2-53fe70203e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p5/t0t3kgq531340_t8gvllvm2m0000gn/T/ipykernel_52673/2980087602.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  state_df[location_id] = state_data[:, count]\n",
      "/var/folders/p5/t0t3kgq531340_t8gvllvm2m0000gn/T/ipykernel_52673/2980087602.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  state_df[location_id] = state_data[:, count]\n",
      "/var/folders/p5/t0t3kgq531340_t8gvllvm2m0000gn/T/ipykernel_52673/2980087602.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  state_df[location_id] = state_data[:, count]\n",
      "/var/folders/p5/t0t3kgq531340_t8gvllvm2m0000gn/T/ipykernel_52673/2980087602.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  state_df[location_id] = state_data[:, count]\n",
      "/var/folders/p5/t0t3kgq531340_t8gvllvm2m0000gn/T/ipykernel_52673/2980087602.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  state_df[location_id] = state_data[:, count]\n"
     ]
    }
   ],
   "source": [
    "for year in range(start, end):\n",
    "    # Set remote destination of h5 file from nrel\n",
    "    file_path = \"/nrel/nsrdb/v3/nsrdb_{}.h5\".format(year)\n",
    "    with h5pyd.File(file_path, mode=\"r\") as f:\n",
    "        # create time indices for dataframe\n",
    "        time_df = pd.DataFrame()\n",
    "        time_df[\"datetime\"] = pd.to_datetime(f[\"time_index\"][::2].astype(str))\n",
    "        time_df[\"time\"] = [d.time() for d in time_df[\"datetime\"]]\n",
    "\n",
    "        # for each type of data (https://nsrdb.nrel.gov/about/u-s-data.html)\n",
    "        for dset in [\n",
    "            \"ghi\"\n",
    "        ]:  # ['dni', 'wind_speed', 'wind_direction', 'dhi', 'air_temperature', 'solar_zenith_angle']:\n",
    "            # access the nsrdb h5 file\n",
    "            ds = f[dset]\n",
    "            state_df = pd.DataFrame()\n",
    "\n",
    "            # create numpy superset array containing all columns for current state\n",
    "            lower_bound = state_index[0]\n",
    "            upper_bound = state_index[-1]\n",
    "            supset = (\n",
    "                ds[::2, lower_bound : upper_bound + 1] / ds.attrs[\"psm_scale_factor\"]\n",
    "            )  # ::2 ---> hourly data only\n",
    "\n",
    "            # Determine which columns to remove from superset of columns\n",
    "            offset = state_index[0]\n",
    "            state_list = [x - offset for x in state_index]\n",
    "            remove_indices = find_missing(state_list)\n",
    "            state_data = np.delete(supset, remove_indices, 1)\n",
    "\n",
    "            # Add station labels to data\n",
    "            count = 0\n",
    "            for index in state_index:\n",
    "                location_id = f\"station_{index:0=9d}\"\n",
    "                state_df[location_id] = state_data[:, count]\n",
    "                count += 1\n",
    "\n",
    "            # concatenate state data with timeseries indices\n",
    "            time_state_df = pd.concat([time_df, state_df], axis=1)\n",
    "\n",
    "            # removing all data except specified UTC time\n",
    "            state_noon_data = time_state_df[\n",
    "                time_state_df[\"time\"].astype(str).isin([utctime + \":00:00\"])\n",
    "            ]\n",
    "            del state_noon_data[\"time\"]\n",
    "\n",
    "            # reshape data to columnar df\n",
    "            melted_state_df = pd.melt(\n",
    "                state_noon_data,\n",
    "                id_vars=[\"datetime\"],\n",
    "                var_name=\"Station\",\n",
    "                value_name=dset,\n",
    "            )\n",
    "\n",
    "            final_state_df = melted_state_df[\n",
    "                melted_state_df[\"Station\"].isin(final_latlon[\"Station\"])\n",
    "            ]\n",
    "\n",
    "            feather.write_feather(\n",
    "                final_state_df, f\"./data/nsrdb_{year}_{state}_{utctime}UTC_GHI.feather\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
