{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATOR parameters\n",
    "\n",
    "# length of array in arrFloat column\n",
    "arr_length = 1000\n",
    "\n",
    "# number of rows in the partition 0\n",
    "nb_rows = 10000\n",
    "\n",
    "# number of partition :\n",
    "# each partition is copied and modified from the previous tableset and appended to it\n",
    "# max number of copied rows is 200_000\n",
    "nb_part = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Dataframe generation with 10,000 rows x 100 cols and a 1000-array col ...  --> done in 0.53 sec\n",
      " --> Memory used = 108.5 MBytes\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# number of distinct values in each data type\n",
    "nb_cats = min(int(nb_rows**0.5), 100)\n",
    "shuffle_cols = False\n",
    "\n",
    "# Generate Dataframe\n",
    "nb_cols_int, nb_cols_date, nb_cols_str = 34, 33, 33\n",
    "\n",
    "table_spec = dict(\n",
    "    **{f\"int{i}\": \"int\" for i in range(nb_cols_int)},\n",
    "    **{f\"dttime{i}\": \"datetime\" for i in range(nb_cols_date)},\n",
    "    **{f\"str{i}\": \"str\" for i in range(nb_cols_str)},\n",
    ")\n",
    "\n",
    "# Generate first partition of data (partition=0) in Dafaframe, without array column\n",
    "myrnd = random.Random(0)\n",
    "np.random.seed(0)\n",
    "myrnd.seed(0)\n",
    "t0 = time()\n",
    "print(\n",
    "    f\" --> Dataframe generation with {nb_rows:,d} rows x {len(table_spec)} cols and {'no ' if arr_length==0 else f'a {arr_length}-'}array col ... \",\n",
    "    end=\"\",\n",
    ")\n",
    "max_int = 10**6\n",
    "date_min = pd.to_datetime(\"2018-01-01\")\n",
    "date_max = pd.to_datetime(\"2050-12-31\")\n",
    "\n",
    "\n",
    "# Create categorical values for each data type ie. decoration or attributes\n",
    "chars = string.ascii_letters\n",
    "cats = {\n",
    "    # Generate a list of nb_cats random-sized strings built from random letters & digits\n",
    "    \"str\": [\n",
    "        \"\".join(myrnd.choice(chars) for _ in range(x))\n",
    "        for x in np.random.randint(5, 20, size=nb_cats)\n",
    "    ],\n",
    "    # Generate a list of nb_cats random int64s\n",
    "    \"int\": np.random.randint(-max_int, max_int, size=nb_cats, dtype=np.int64),\n",
    "    # Generate a list of nb_cats random DateTimes\n",
    "    \"datetime\": (\n",
    "        np.random.randint(\n",
    "            date_min.value // 10**9,\n",
    "            date_max.value // 10**9,\n",
    "            size=nb_cats,\n",
    "            dtype=np.int64,\n",
    "        )\n",
    "    )\n",
    "    * 10**9,\n",
    "}\n",
    "\n",
    "# Generate data\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        name: (\n",
    "            np.random.choice(cats[typ], size=nb_rows)\n",
    "            if typ in [\"str\", \"int\"]\n",
    "            else np.random.choice(cats[typ], size=nb_rows).view(\"M8[ns]\")\n",
    "            if typ == \"datetime\"\n",
    "            else np.nan\n",
    "        )\n",
    "        for name, typ in table_spec.items()\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "# Generate arrays column\n",
    "if arr_length > 0:\n",
    "    result[\"arrFloat\"] = (np.random.rand(nb_rows, arr_length) * 1e6).tolist()\n",
    "\n",
    "# Add a partition column\n",
    "result[\"partition\"] = 0\n",
    "t1 = time()\n",
    "print(f\" --> done in {t1-t0:.2f} sec\")\n",
    "print(f\" --> Memory used = {result.memory_usage(deep=True).sum()/1e6:,.1f} MBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Duplicated dataset generated with 1,720,000 rows x 100 cols and a 1000-array col ... \n"
     ]
    }
   ],
   "source": [
    "# Dataset duplication, each partition recursively duplicates\n",
    "# already accumulated rows (with a maximum of 200000 rows)\n",
    "for part in range(1, nb_part):\n",
    "    duplicate = result.copy().tail(200000)\n",
    "    duplicate[\"partition\"] = part\n",
    "    result = pd.concat([result, duplicate])\n",
    "\n",
    "print(\n",
    "    f\" --> Duplicated dataset generated with {len(result.index):,d} rows x {len(table_spec)} cols and {'no ' if arr_length==0 else f'a {arr_length}-'}array col ... \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to a CSV file\n",
    "result.to_csv(\"dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dataframe to a parquet file,\n",
    "# using pyarrow to specify the array column as float[].\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "schema = pa.Table.from_pandas(result, preserve_index=False).schema\n",
    "new_schema = pa.schema(\n",
    "    [\n",
    "        f if f.name != \"arrFloat\" else pa.field(\"arrFloat\", pa.list_(pa.float32()))\n",
    "        for f in schema\n",
    "    ]\n",
    ")\n",
    "result.to_parquet(\n",
    "    \"dataset.parquet\", index=False, schema=new_schema, row_group_size=10000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
