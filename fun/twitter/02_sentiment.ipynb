{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you haven't had nltk and textblob installed\n",
    "# !pip install nltk textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sentiment  \n",
    "\n",
    "In the first [notebook](01_twitter_data_scraping.ipynb), we have downloaded tweets for different cryptocurrencies. We will next perform [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis#:~:text=Sentiment%20analysis%20(also%20known%20as,affective%20states%20and%20subjective%20information.) on the tweets so as to understand the mood of the public towards each cryptocurrency, and eventually see if the sentiments could be used to predict the returns of the cryptocurrency.\n",
    "\n",
    "As a quickstart to [Natural Language Programming](https://en.wikipedia.org/wiki/Natural-language_programming), we will be using [TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html) to extract the polarity of the tweets. We will revisit polarity again later in the notebook.  \n",
    "\n",
    "Do note that we are unable to share the downloaded tweets due to Twitter's regulation on its data. Hence this notebook will only serve as a reference on how we can obtain the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the mined tweets are split into files by the currency, the below list is used to iterate through the currency files for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency = [\n",
    "    \"ADA\",\n",
    "    \"BSV\",\n",
    "    \"LTC\",\n",
    "    \"LINK\",\n",
    "    \"BNB\",\n",
    "    \"BTC\",\n",
    "    \"ETH\",\n",
    "    \"XRP\",\n",
    "    \"USDT\",\n",
    "    \"BCH\",\n",
    "    \"EOS\",\n",
    "    \"TRON\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ccy(curr):\n",
    "    pd_df = pd.concat(\n",
    "        [pd.read_csv(f) for f in glob.glob(f\"clean_data/{curr}*.csv\", recursive=True)],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of records loaded for {curr}\", len(pd_df))\n",
    "    pd_df[[\"coin_symbol\", \"tweet_id\", \"created_at\", \"date\", \"hour\"]].to_csv(\n",
    "        f\"clean_data/ccy_tweets/{curr}_tweets.csv\", index=False\n",
    "    )\n",
    "\n",
    "    return pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine all the downloaded tweets into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(\n",
    "    [process_ccy(c) for c in currency],\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we repeated the data mining multiple times, we expect duplication of tweets. We will keep the latest mined as the number of followers and retweets can change.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sort_values(by=[\"mined_at\"], inplace=True, ignore_index=True)\n",
    "combined_df.drop_duplicates(\n",
    "    subset=[\"tweet_id\"], inplace=True, keep=\"last\", ignore_index=True\n",
    ")\n",
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `tweet_id` is somewhat similar to timestamp, we sort the tweets by the id to get it in chronological order (though it doesn't really matter much here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sort_values(by=[\"tweet_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We used the minimal text processing in this notebook to turn the text into a canonical, machine-readable format.  \n",
    "The text normalization process will include:\n",
    "- converting all letters into lower case\n",
    "- Lemmatization and stemming the words \n",
    "- remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    words = [lemmatizer.lemmatize(w) for w in text if w not in stop_words]\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in words])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"processed text\"] = combined_df[\"text\"].apply(\n",
    "    lambda x: preprocess(tokenizer.tokenize(x.lower()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting sentiment\n",
    "\n",
    "We create a TextBlob and obtain the polarity from its [sentiment](https://textblob.readthedocs.io/en/dev/api_reference.html#textblob.blob.TextBlob.sentiment) property.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    return analysis.sentiment.polarity  # work out sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"polarity\"] = combined_df[\"processed text\"].apply(lambda x: getSentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polarity is a value within -1.0 and 1.0 where 0 shows a neutral post, and likely to be a bot posting.  \n",
    "Negative polarity indicates a negative posting and likewise, postive polarity indicates a positive mood.  \n",
    "We create a `sentiment` column to store these classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"sentiment\"] = combined_df[\"polarity\"].apply(\n",
    "    lambda s: \"Postive\" if s > 0 else (\"Neutral\" if s == 0 else \"Negative\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"full_tweets.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
